{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b369a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shape: (99300, 5)\n",
      "HOEP                  0\n",
      "Hour 1 Predispatch    0\n",
      "Hour 2 Predispatch    0\n",
      "Hour 3 Predispatch    0\n",
      "Ontario Demand        0\n",
      "dtype: int64\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read and concat yearly files\n",
    "price_list, demand_list = [], []\n",
    "for year in range(2014, 2026):\n",
    "    p = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_PriceHOEPPredispOR_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    d = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_Demand_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    price_list.append(p); demand_list.append(d)\n",
    "\n",
    "price_df  = pd.concat(price_list,  ignore_index=True)\n",
    "demand_df = pd.concat(demand_list, ignore_index=True)\n",
    "\n",
    "# Clean names\n",
    "price_df.columns  = price_df.columns.str.strip()\n",
    "demand_df.columns = demand_df.columns.str.strip()\n",
    "\n",
    "price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']] = \\\n",
    "    price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']].ffill() # We use forward fill for NaNs as they appear in <1 percent of the data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df in (price_df, demand_df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Combine Date and Hour into naive datetime\n",
    "    naive_ts = df['Date'] + pd.to_timedelta(df['Hour'] - 1, unit='h')\n",
    "\n",
    "    # Localize with correct DST handling\n",
    "    df['timestamp'] = naive_ts.dt.tz_localize(\n",
    "        'Canada/Eastern',\n",
    "        ambiguous=False,               # Convert to UTC to handle day light saving shifts in dataFrame\n",
    "        nonexistent='shift_forward'   \n",
    "    ).dt.tz_convert('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# Drop duplicates from daylight savings\n",
    "price_df  = price_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "demand_df = demand_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "# Merge price + demand on timestamp\n",
    "combined_df = pd.merge(\n",
    "    price_df, demand_df,\n",
    "    on='timestamp', how='inner',\n",
    "    suffixes=('_price','_demand')\n",
    ")\n",
    "combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# 6. Cleanup\n",
    "combined_df = combined_df.drop(  # Drop non-live compatible features and duplicates columns\n",
    "    columns=[\n",
    "        'Date_price','Hour_price',\n",
    "        'Date_demand','Hour_demand',\n",
    "        'OR 10 Min Sync','OR 10 Min non-sync','OR 30 Min', 'Market Demand'\n",
    "    ],\n",
    "    errors='ignore'\n",
    ")\n",
    "combined_df = combined_df.set_index('timestamp')\n",
    "\n",
    "\n",
    "print(f\"\\nFinal shape: {combined_df.shape}\")\n",
    "print(combined_df.isna().sum())\n",
    "print(\"Range:\", combined_df.index.min(), \"→\", combined_df.index.max()) # Final output (number of NaNs for each column, range, and shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40ef85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (99300, 12)\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load weather folder paths\n",
    "root = r'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\weather'\n",
    "cities = ['toronto','kitchener','london','ottawa']\n",
    "col_map = {\n",
    "    \"Temp (°C)\":    \"temp\",\n",
    "    \"Rel Hum (%)\":  \"humidity\",\n",
    "    \"Wind Spd (km/h)\": \"wind_speed\",\n",
    "}\n",
    "\n",
    "city_dfs = []\n",
    "for city in cities:\n",
    "    path = os.path.join(root, city) # Becomes raw/weather/toronto , kitchener etc..\n",
    "    dfs = []\n",
    "    for fname in sorted(os.listdir(path)):  # Sort from 2014-2025\n",
    "        df = pd.read_csv(os.path.join(path, fname))\n",
    "        df.columns = df.columns.str.strip().str.replace('\"','') # Clean columns\n",
    "\n",
    "        naive_ts = pd.to_datetime(df['Date/Time (LST)'], errors='coerce')\n",
    "\n",
    "        df = df[list(col_map)].rename(columns=col_map) # Rename wanted columns and drop unwanted ones\n",
    "\n",
    "\n",
    "        df['timestamp'] = naive_ts.dt.tz_localize('Canada/Eastern', ambiguous=False, nonexistent='shift_forward').dt.tz_convert('UTC') # UTC timestamp \n",
    "        dfs.append(df)\n",
    "    \n",
    "   \n",
    "    city_df = pd.concat(dfs, ignore_index=True)\n",
    "    city_df  = city_df.drop_duplicates(subset=['timestamp']) # Drop daylight saving timestamps, we use shift forward to create duplicates and then drop them\n",
    "    city_df = city_df.set_index('timestamp').sort_index()\n",
    "    \n",
    "    city_df = city_df.ffill() # We use forward fill as close to 1 percent data missing. Ffil is sufficent\n",
    "    city_df = city_df.add_suffix(f\"_{city}\")\n",
    "    city_dfs.append(city_df) # Contains each cities sorted cleaned and filled df\n",
    "\n",
    "# inner join across all cities (keeps only timestamps present in every city)\n",
    "from functools import reduce\n",
    "weather_df = reduce(lambda L, R: L.join(R, how='inner'), city_dfs)\n",
    "weather_df = weather_df.sort_index()\n",
    "\n",
    "print(\"Shape:\", weather_df.shape)\n",
    "print(\"Range:\", weather_df.index.min(), \"→\", weather_df.index.max())  # Shape and ranges matches previous cell df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8af0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged shape: (99300, 17)\n",
      "Merged range: 2014-01-01 05:00:00+00:00 2025-05-01 03:00:00+00:00\n",
      "HOEP                    0\n",
      "Hour 1 Predispatch      0\n",
      "Hour 2 Predispatch      0\n",
      "Hour 3 Predispatch      0\n",
      "Ontario Demand          0\n",
      "temp_toronto            0\n",
      "humidity_toronto        0\n",
      "wind_speed_toronto      0\n",
      "temp_kitchener          0\n",
      "humidity_kitchener      0\n",
      "wind_speed_kitchener    0\n",
      "temp_london             0\n",
      "humidity_london         0\n",
      "wind_speed_london       0\n",
      "temp_ottawa             0\n",
      "humidity_ottawa         0\n",
      "wind_speed_ottawa       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Both have 'timestamp' index in UTC now, accounts for daylight savings\n",
    "final_df = combined_df.join(weather_df, how='inner')\n",
    "\n",
    "print(\"\\nMerged shape:\", final_df.shape)\n",
    "print(\"Merged range:\", final_df.index.min(), final_df.index.max())\n",
    "\n",
    "\n",
    "print(final_df.isna().sum())\n",
    "# Final Nan check , range and shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b7696d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (99285, 17)\n",
      "['HOEP', 'Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch', 'Ontario Demand', 'price_forecast_error_h1', 'price_forecast_error_h2', 'price_forecast_error_h3', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'HOEP_lag_1', 'HOEP_lag_2', 'HOEP_lag_3', 'HOEP_lag_6', 'HOEP_lag_12']\n"
     ]
    }
   ],
   "source": [
    "# We create the Dataframe for the ELM. We use a minimum shift of 2 on all features except IESO predispatch and time features. These are forecasts known at the time of prediction\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Config\n",
    "hoep_lags = [1, 2, 3, 6, 12]\n",
    "\n",
    "# Reset index to create time features using timestamp\n",
    "\n",
    "PDP_df = final_df.copy() # Copy unshifted DF for PDP metrics\n",
    "df = final_df.copy()\n",
    "df = df.reset_index()\n",
    "\n",
    "# Convert from string to numeric\n",
    "df['HOEP'] = pd.to_numeric(df['HOEP'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 1 Predispatch'] = pd.to_numeric(df['Hour 1 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 2 Predispatch'] = pd.to_numeric(df['Hour 2 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 3 Predispatch'] = pd.to_numeric(df['Hour 3 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "\n",
    "\n",
    "df['Hour 1 Predispatch'] = df['Hour 1 Predispatch'].shift(-1)\n",
    "df['Hour 2 Predispatch'] = df['Hour 2 Predispatch'].shift(-2)\n",
    "df['Hour 3 Predispatch'] = df['Hour 3 Predispatch'].shift(-3)\n",
    "\n",
    "df['price_forecast_error_h1'] = df['HOEP'] - df['Hour 1 Predispatch'].shift(1)\n",
    "df['price_forecast_error_h2'] = df['HOEP'] - df['Hour 2 Predispatch'].shift(2)  \n",
    "df['price_forecast_error_h3'] = df['HOEP'] - df['Hour 3 Predispatch'].shift(3)\n",
    "\n",
    "# Change to datetime object for feature engineering\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Time features\n",
    "df['month'] = df['timestamp'].dt.month \n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lag_features = {}\n",
    "for k in hoep_lags:\n",
    "    lag_features[f'HOEP_lag_{k}']= df['HOEP'].shift(k)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(lag_features, index=df.index)], axis=1)\n",
    "\n",
    "# Final cleanup\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop leakage columns\n",
    "cols_to_drop = [      \n",
    "   'hour', \n",
    "   'month',\n",
    "   'temp_toronto',\n",
    "   'wind_speed_toronto', \n",
    "   'humidity_toronto',\n",
    "   'temp_london',\n",
    "   'wind_speed_london',\n",
    "   'humidity_london', \n",
    "   'temp_kitchener',\n",
    "   'wind_speed_kitchener',\n",
    "   'humidity_kitchener',\n",
    "   'temp_ottawa',\n",
    "   'wind_speed_ottawa',\n",
    "   'humidity_ottawa'\n",
    "]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "features = df.columns.tolist()\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e4058d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-horizon targets (t+1, t+2, t+3)\n",
    "df['target_t1'] = df['HOEP'].shift(-1)  # 1 hour ahead\n",
    "df['target_t2'] = df['HOEP'].shift(-2)  # 2 hours ahead  \n",
    "df['target_t3'] = df['HOEP'].shift(-3)  # 3 hours ahead\n",
    "\n",
    "targets = ['target_t1', 'target_t2', 'target_t3']\n",
    "\n",
    "df = df.dropna(subset=targets)\n",
    "\n",
    "# 3-way split for proper validation\n",
    "train_start = pd.Timestamp(\"2014-01-01 00:00:00\", tz=\"UTC\")\n",
    "train_cutoff = pd.Timestamp(\"2023-01-01 00:00:00\", tz=\"UTC\")\n",
    "val_cutoff = pd.Timestamp(\"2024-01-01 00:00:00\", tz=\"UTC\")\n",
    "final_cutoff = pd.Timestamp(\"2025-01-01 00:00:00\", tz='UTC')\n",
    "\n",
    "df_train = df[(df.index < train_cutoff)] # Train on 2014-2022\n",
    "df_val = df[(df.index >= train_cutoff) & (df.index < val_cutoff)]  # Validate on 2023\n",
    "df_test = df[(df.index >= val_cutoff ) & (df.index < final_cutoff)] # Test on 2024\n",
    "\n",
    "\n",
    "X_train = df_train[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_train = df_train[targets].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "X_val = df_val[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_val = df_val[targets].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "X_test = df_test[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_test = df_test[targets].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Save column names before scaling\n",
    "feature_names_ELM = X_train.columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a30b31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing with k=17 features \n",
      "Hyperparameter Tuning Results:\n",
      "n_hidden=100: Overall Val RMSE = 16.27, Train Time = 1.424s\n",
      "n_hidden=250: Overall Val RMSE = 15.91, Train Time = 3.640s\n",
      "n_hidden=500: Overall Val RMSE = 15.78, Train Time = 8.476s\n",
      "n_hidden=750: Overall Val RMSE = 15.98, Train Time = 14.266s\n",
      "n_hidden=1000: Overall Val RMSE = 16.03, Train Time = 23.691s\n",
      "\n",
      "Final Best ELM Model:\n",
      "Best k (num features): 17\n",
      "Best n_hidden        : 500\n",
      "Best validation RMSE : 15.78\n",
      "Train Time           : 8.476s\n",
      "\n",
      "FINAL MODEL EVALUATION\n",
      "\n",
      "Final Test Results:\n",
      "t+1 - RMSE: 22.49, MAE: 7.78, R²: 0.308\n",
      "t+2 - RMSE: 23.89, MAE: 8.82, R²: 0.220\n",
      "t+3 - RMSE: 24.42, MAE: 9.28, R²: 0.185\n",
      "Overall - RMSE: 23.60, MAE: 8.63, R²: 0.238\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def calculate_multi_horizon_metrics(y_true, y_pred, horizon_names=['t+1', 't+2', 't+3']):\n",
    "    metrics = {}\n",
    "    \n",
    "    for i, horizon in enumerate(horizon_names):\n",
    "        y_true_horizon = y_true.iloc[:, i] if hasattr(y_true, 'iloc') else y_true[:, i]\n",
    "        y_pred_horizon = y_pred[:, i] if y_pred.ndim > 1 else y_pred\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_horizon, y_pred_horizon)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_horizon, y_pred_horizon))\n",
    "        mape = np.mean(np.abs((y_true_horizon - y_pred_horizon) / y_true_horizon)) * 100\n",
    "        r2 = r2_score(y_true_horizon, y_pred_horizon)\n",
    "        \n",
    "        metrics[f'{horizon}_MAE'] = mae\n",
    "        metrics[f'{horizon}_RMSE'] = rmse\n",
    "        metrics[f'{horizon}_MAPE'] = mape\n",
    "        metrics[f'{horizon}_R2'] = r2\n",
    "    \n",
    "    overall_mae = np.mean([metrics[f'{h}_MAE'] for h in horizon_names])\n",
    "    overall_rmse = np.mean([metrics[f'{h}_RMSE'] for h in horizon_names])\n",
    "    overall_r2 = np.mean([metrics[f'{h}_R2'] for h in horizon_names])\n",
    "    \n",
    "    metrics['Overall_MAE'] = overall_mae\n",
    "    metrics['Overall_RMSE'] = overall_rmse\n",
    "    metrics['Overall_R2'] = overall_r2\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Metrics\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    \n",
    "    horizons = ['t+1', 't+2', 't+3']\n",
    "    for horizon in horizons:\n",
    "        print(f\"{horizon} - RMSE: {metrics[f'{horizon}_RMSE']:.2f}, \"\n",
    "              f\"MAE: {metrics[f'{horizon}_MAE']:.2f}, \"\n",
    "              f\"R²: {metrics[f'{horizon}_R2']:.3f}\")\n",
    "    \n",
    "    print(f\"Overall - RMSE: {metrics['Overall_RMSE']:.2f}, \"\n",
    "          f\"MAE: {metrics['Overall_MAE']:.2f}, \"\n",
    "          f\"R²: {metrics['Overall_R2']:.3f}\")\n",
    "\n",
    "best_rmse = float('inf')\n",
    "best_n_hidden = None\n",
    "best_k = None\n",
    "best_selector = None\n",
    "best_train_time = None\n",
    "best_scaler = None\n",
    "best_selected_feature_names = None\n",
    "\n",
    "for k in [len(X_train.columns)]:\n",
    "    print(f\"\\n Testing with k={k} features \")\n",
    "    \n",
    "    if k == len(X_train.columns):\n",
    "        X_train_sel_raw = X_train\n",
    "        X_val_sel_raw = X_val\n",
    "        X_test_sel_raw = X_test\n",
    "        selector = None\n",
    "        selected_features = X_train.columns\n",
    "    else:\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        y_train_avg = y_train.mean(axis=1)\n",
    "        selector.fit(X_train, y_train_avg)\n",
    "        \n",
    "        X_train_sel_raw = selector.transform(X_train)\n",
    "        X_val_sel_raw = selector.transform(X_val)\n",
    "        X_test_sel_raw = selector.transform(X_test)\n",
    "        selected_features = X_train.columns[selector.get_support()]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_sel = scaler.fit_transform(X_train_sel_raw)\n",
    "    X_val_sel = scaler.transform(X_val_sel_raw)\n",
    "    X_test_sel = scaler.transform(X_test_sel_raw)\n",
    "    \n",
    "    print(\"Hyperparameter Tuning Results:\")\n",
    "    \n",
    "    for n_hidden in [100, 250, 500, 750, 1000]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        W_in = np.random.randn(X_train_sel.shape[1], n_hidden)\n",
    "        H_train = np.maximum(0, X_train_sel @ W_in)\n",
    "        H_val = np.maximum(0, X_val_sel @ W_in)\n",
    "        \n",
    "        W_out = np.linalg.pinv(H_train) @ y_train.values\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        y_val_pred = H_val @ W_out\n",
    "        \n",
    "        val_metrics = calculate_multi_horizon_metrics(y_val, y_val_pred)\n",
    "        val_rmse = val_metrics['Overall_RMSE']\n",
    "        \n",
    "        print(f\"n_hidden={n_hidden}: Overall Val RMSE = {val_rmse:.2f}, Train Time = {train_time:.3f}s\")\n",
    "        \n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_n_hidden = n_hidden\n",
    "            best_k = k\n",
    "            best_selector = selector\n",
    "            best_scaler = scaler\n",
    "            best_train_time = train_time\n",
    "            best_selected_feature_names = selected_features\n",
    "\n",
    "print(\"\\nFinal Best ELM Model:\")\n",
    "print(f\"Best k (num features): {best_k}\")\n",
    "print(f\"Best n_hidden        : {best_n_hidden}\")\n",
    "print(f\"Best validation RMSE : {best_rmse:.2f}\")\n",
    "print(f\"Train Time           : {best_train_time:.3f}s\")\n",
    "\n",
    "\n",
    "print(\"\\nFINAL MODEL EVALUATION\")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "if best_selector is not None:\n",
    "    X_train_sel_raw = best_selector.transform(X_train)\n",
    "    X_test_sel_raw = best_selector.transform(X_test)\n",
    "else:\n",
    "    X_train_sel_raw = X_train\n",
    "    X_test_sel_raw = X_test\n",
    "\n",
    "X_train_sel = best_scaler.fit_transform(X_train_sel_raw)\n",
    "X_test_sel = best_scaler.transform(X_test_sel_raw)\n",
    "\n",
    "W_in = np.random.randn(X_train_sel.shape[1], best_n_hidden)\n",
    "H_train = np.maximum(0, X_train_sel @ W_in)\n",
    "H_test = np.maximum(0, X_test_sel @ W_in)\n",
    "W_out = np.linalg.pinv(H_train) @ y_train.values\n",
    "\n",
    "y_test_pred = H_test @ W_out\n",
    "\n",
    "test_metrics = calculate_multi_horizon_metrics(y_test, y_test_pred)\n",
    "print_metrics(test_metrics, \"Final Test Results\")\n",
    "\n",
    "model_data = {\n",
    "    \"W_in\": W_in,\n",
    "    \"W_out\": W_out,\n",
    "    \"n_hidden\": best_n_hidden,\n",
    "    \"scaler\": best_scaler,\n",
    "    \"selector\": best_selector,\n",
    "    \"feature_names\": best_selected_feature_names,\n",
    "    \"horizons\": ['t+1', 't+2', 't+3']\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, \"elm_multi_horizon_model.pkl\")\n",
    "\n",
    "\n",
    "prediction_results = {}\n",
    "for i, horizon in enumerate(['t+1', 't+2', 't+3']):\n",
    "    y_test_horizon = pd.Series(y_test.iloc[:, i].values, index=df_test.index, name=f'Actual_{horizon}')\n",
    "    y_pred_horizon = pd.Series(y_test_pred[:, i], index=df_test.index, name=f'Predicted_{horizon}')\n",
    "    \n",
    "    prediction_results[horizon] = {\n",
    "        'actual': y_test_horizon,\n",
    "        'predicted': y_pred_horizon\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7a5adf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model successfully exported to elm_model_extracted.json\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved ELM model\n",
    "model = joblib.load(\"elm_multi_horizon_model.pkl\")\n",
    "\n",
    "# Extract and convert weights + scaler\n",
    "W_in = model[\"W_in\"].tolist()         # shape: [n_features, n_hidden]\n",
    "W_out = model[\"W_out\"].tolist()       # shape: [n_hidden] or [n_hidden, 1]\n",
    "scaler_mean = model[\"scaler\"].mean_.tolist()\n",
    "scaler_scale = model[\"scaler\"].scale_.tolist()\n",
    "n_hidden = model[\"n_hidden\"]\n",
    "\n",
    "# Prepare dictionary\n",
    "model_json = {\n",
    "    \"W_in\": W_in,\n",
    "    \"W_out\": W_out,\n",
    "    \"scaler_mean\": scaler_mean,\n",
    "    \"scaler_scale\": scaler_scale,\n",
    "    \"n_hidden\": n_hidden\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"elm_model_extracted.json\", \"w\") as f:\n",
    "    json.dump(model_json, f)\n",
    "\n",
    "print(\" Model successfully exported to elm_model_extracted.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e7c69917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing LSTM data using same train/val/test splits as ELM...\n",
      "Train period: 2014-01-01 17:00:00+00:00 to 2022-12-31 23:00:00+00:00\n",
      "Val period: 2023-01-01 00:00:00+00:00 to 2023-12-31 23:00:00+00:00\n",
      "Test period: 2024-01-01 00:00:00+00:00 to 2024-12-31 23:00:00+00:00\n",
      "Train shape: (78859, 20)\n",
      "Val shape: (8756, 20)\n",
      "Test shape: (8780, 20)\n",
      "Sequence shapes:\n",
      "X_train_seq: (78847, 12, 12)\n",
      "y_train_seq: (78847, 3)\n",
      "X_val_seq: (8744, 12, 12)\n",
      "X_test_seq: (8768, 12, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\Documents\\Projects\\research\\.venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">99</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m19,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m99\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,891</span> (85.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,891\u001b[0m (85.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,891</span> (85.51 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,891\u001b[0m (85.51 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 12ms/step - loss: 1.0614 - mae: 0.4158 - val_loss: 0.2417 - val_mae: 0.2111 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 20ms/step - loss: 0.8186 - mae: 0.3346 - val_loss: 0.2350 - val_mae: 0.2003 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 19ms/step - loss: 0.7917 - mae: 0.3234 - val_loss: 0.2361 - val_mae: 0.2057 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 21ms/step - loss: 0.7785 - mae: 0.3171 - val_loss: 0.2344 - val_mae: 0.2036 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - loss: 0.7716 - mae: 0.3153 - val_loss: 0.2328 - val_mae: 0.1981 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 15ms/step - loss: 0.7522 - mae: 0.3099 - val_loss: 0.2360 - val_mae: 0.2050 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 15ms/step - loss: 0.7473 - mae: 0.3105 - val_loss: 0.2370 - val_mae: 0.2047 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 16ms/step - loss: 0.7420 - mae: 0.3085 - val_loss: 0.2399 - val_mae: 0.2065 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 10ms/step - loss: 0.7366 - mae: 0.3086 - val_loss: 0.2379 - val_mae: 0.2067 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m1230/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.7349 - mae: 0.3088\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.7346 - mae: 0.3087 - val_loss: 0.2384 - val_mae: 0.2036 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 12ms/step - loss: 0.7216 - mae: 0.3018 - val_loss: 0.2380 - val_mae: 0.2032 - learning_rate: 5.0000e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 11ms/step - loss: 0.7080 - mae: 0.3001 - val_loss: 0.2369 - val_mae: 0.2027 - learning_rate: 5.0000e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.6995 - mae: 0.2992 - val_loss: 0.2372 - val_mae: 0.2030 - learning_rate: 5.0000e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 9ms/step - loss: 0.6963 - mae: 0.2974 - val_loss: 0.2389 - val_mae: 0.2033 - learning_rate: 5.0000e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m1230/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6940 - mae: 0.2989\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m1232/1232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 13ms/step - loss: 0.6937 - mae: 0.2988 - val_loss: 0.2367 - val_mae: 0.2020 - learning_rate: 5.0000e-04\n",
      "Epoch 15: early stopping\n",
      "Restoring model weights from the end of the best epoch: 5.\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "LSTM Validation Results:\n",
      "==================================================\n",
      "t+1 - RMSE: 16.01, MAE: 6.27, R²: 0.396\n",
      "t+2 - RMSE: 16.46, MAE: 6.77, R²: 0.362\n",
      "t+3 - RMSE: 16.95, MAE: 7.25, R²: 0.324\n",
      "--------------------------------------------------\n",
      "Overall - RMSE: 16.47, MAE: 6.76, R²: 0.361\n",
      "\n",
      "LSTM Test Results:\n",
      "==================================================\n",
      "t+1 - RMSE: 23.73, MAE: 8.09, R²: 0.231\n",
      "t+2 - RMSE: 24.20, MAE: 8.60, R²: 0.201\n",
      "t+3 - RMSE: 24.53, MAE: 9.04, R²: 0.178\n",
      "--------------------------------------------------\n",
      "Overall - RMSE: 24.15, MAE: 8.58, R²: 0.203\n",
      "\n",
      "Training Summary:\n",
      "Train Time: 261.79 seconds\n",
      "Total Epochs: 15\n",
      "Best Val Loss: 0.2328\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "n_steps = 12\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "lstm_units = 64\n",
    "\n",
    "print(\"Preparing LSTM data using same train/val/test splits as ELM...\")\n",
    "print(f\"Train period: {df_train.index.min()} to {df_train.index.max()}\")\n",
    "print(f\"Val period: {df_val.index.min()} to {df_val.index.max()}\")\n",
    "print(f\"Test period: {df_test.index.min()} to {df_test.index.max()}\")\n",
    "\n",
    "# Create multi-horizon targets\n",
    "def create_multi_horizon_targets(df_subset):\n",
    "    df_copy = df_subset.copy()\n",
    "    df_copy['target_t1'] = df_copy['HOEP'].shift(-1)\n",
    "    df_copy['target_t2'] = df_copy['HOEP'].shift(-2)\n",
    "    df_copy['target_t3'] = df_copy['HOEP'].shift(-3)\n",
    "    df_copy = df_copy.dropna(subset=['target_t1', 'target_t2', 'target_t3'])\n",
    "    return df_copy\n",
    "\n",
    "lstm_train = create_multi_horizon_targets(df_train)\n",
    "lstm_val = create_multi_horizon_targets(df_val)\n",
    "lstm_test = create_multi_horizon_targets(df_test)\n",
    "\n",
    "print(f\"Train shape: {lstm_train.shape}\")\n",
    "print(f\"Val shape: {lstm_val.shape}\")\n",
    "print(f\"Test shape: {lstm_test.shape}\")\n",
    "\n",
    "# Use same features as ELM but exclude HOEP lags for LSTM\n",
    "lag_columns = [col for col in features if 'HOEP_lag_' in col]\n",
    "lstm_features = [col for col in features if col not in ['target_t1', 'target_t2', 'target_t3'] + lag_columns]\n",
    "targets = ['target_t1', 'target_t2', 'target_t3']\n",
    "\n",
    "\n",
    "X_train_lstm_raw = lstm_train[lstm_features].values\n",
    "X_val_lstm_raw = lstm_val[lstm_features].values\n",
    "X_test_lstm_raw = lstm_test[lstm_features].values\n",
    "\n",
    "y_train_lstm = lstm_train[targets].values\n",
    "y_val_lstm = lstm_val[targets].values\n",
    "y_test_lstm = lstm_test[targets].values\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_lstm = feature_scaler.fit_transform(X_train_lstm_raw)\n",
    "X_val_lstm = feature_scaler.transform(X_val_lstm_raw)\n",
    "X_test_lstm = feature_scaler.transform(X_test_lstm_raw)\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "y_train_lstm_scaled = target_scaler.fit_transform(y_train_lstm)\n",
    "y_val_lstm_scaled = target_scaler.transform(y_val_lstm)\n",
    "y_test_lstm_scaled = target_scaler.transform(y_test_lstm)\n",
    "\n",
    "\n",
    "\n",
    "def create_sequences(X, y, n_steps):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(n_steps, len(X)):\n",
    "        X_seq.append(X[i - n_steps:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "\n",
    "\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_lstm, y_train_lstm_scaled, n_steps)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_lstm, y_val_lstm_scaled, n_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_lstm, y_test_lstm_scaled, n_steps)\n",
    "\n",
    "print(f\"Sequence shapes:\")\n",
    "print(f\"X_train_seq: {X_train_seq.shape}\")\n",
    "print(f\"y_train_seq: {y_train_seq.shape}\")\n",
    "print(f\"X_val_seq: {X_val_seq.shape}\")\n",
    "print(f\"X_test_seq: {X_test_seq.shape}\")\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(lstm_units, input_shape=(n_steps, X_train_seq.shape[2]), return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(3)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stop, reduce_lr]\n",
    "\n",
    "print(f\"\\nTraining LSTM model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "\n",
    "y_val_pred_scaled = model.predict(X_val_seq)\n",
    "y_test_pred_scaled = model.predict(X_test_seq)\n",
    "\n",
    "y_val_pred = target_scaler.inverse_transform(y_val_pred_scaled)\n",
    "y_test_pred = target_scaler.inverse_transform(y_test_pred_scaled)\n",
    "\n",
    "y_val_true = target_scaler.inverse_transform(y_val_seq)\n",
    "y_test_true = target_scaler.inverse_transform(y_test_seq)\n",
    "\n",
    "def calculate_multi_horizon_metrics(y_true, y_pred, horizon_names=['t+1', 't+2', 't+3']):\n",
    "    metrics = {}\n",
    "    \n",
    "    for i, horizon in enumerate(horizon_names):\n",
    "        y_true_horizon = y_true[:, i]\n",
    "        y_pred_horizon = y_pred[:, i]\n",
    "        \n",
    "        mae = mean_absolute_error(y_true_horizon, y_pred_horizon)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true_horizon, y_pred_horizon))\n",
    "        r2 = r2_score(y_true_horizon, y_pred_horizon)\n",
    "        \n",
    "        metrics[f'{horizon}_MAE'] = mae\n",
    "        metrics[f'{horizon}_RMSE'] = rmse\n",
    "        metrics[f'{horizon}_R2'] = r2\n",
    "    \n",
    "    overall_mae = np.mean([metrics[f'{h}_MAE'] for h in horizon_names])\n",
    "    overall_rmse = np.mean([metrics[f'{h}_RMSE'] for h in horizon_names])\n",
    "    overall_r2 = np.mean([metrics[f'{h}_R2'] for h in horizon_names])\n",
    "    \n",
    "    metrics['Overall_MAE'] = overall_mae\n",
    "    metrics['Overall_RMSE'] = overall_rmse\n",
    "    metrics['Overall_R2'] = overall_r2\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title=\"Metrics\"):\n",
    "    print(f\"\\n{title}:\")\n",
    "    print(\"=\"*50)\n",
    "    horizons = ['t+1', 't+2', 't+3']\n",
    "    for horizon in horizons:\n",
    "        print(f\"{horizon} - RMSE: {metrics[f'{horizon}_RMSE']:.2f}, \"\n",
    "              f\"MAE: {metrics[f'{horizon}_MAE']:.2f}, \"\n",
    "              f\"R²: {metrics[f'{horizon}_R2']:.3f}\")\n",
    "    \n",
    "    print(\"-\"*50)\n",
    "    print(f\"Overall - RMSE: {metrics['Overall_RMSE']:.2f}, \"\n",
    "          f\"MAE: {metrics['Overall_MAE']:.2f}, \"\n",
    "          f\"R²: {metrics['Overall_R2']:.3f}\")\n",
    "\n",
    "val_metrics = calculate_multi_horizon_metrics(y_val_true, y_val_pred)\n",
    "print_metrics(val_metrics, \"LSTM Validation Results\")\n",
    "\n",
    "test_metrics = calculate_multi_horizon_metrics(y_test_true, y_test_pred)\n",
    "print_metrics(test_metrics, \"LSTM Test Results\")\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Train Time: {train_time:.2f} seconds\")\n",
    "print(f\"Total Epochs: {len(history.history['loss'])}\")\n",
    "print(f\"Best Val Loss: {min(history.history['val_loss']):.4f}\")\n",
    "\n",
    "print(f\"\\nSaving model...\")\n",
    "model.save(\"lstm_multi_horizon_model.keras\")\n",
    "\n",
    "\n",
    "lstm_results = {\n",
    "    'val_metrics': val_metrics,\n",
    "    'test_metrics': test_metrics,\n",
    "    'train_time': train_time,\n",
    "    'model_params': {\n",
    "        'n_steps': n_steps,\n",
    "        'lstm_units': lstm_units,\n",
    "        'epochs_trained': len(history.history['loss']),\n",
    "        'batch_size': batch_size\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9810e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hour 1 Predispatch RMSE: 32.14 CAD/MWh\n",
      "Hour 1 Predispatch MAE : 11.68 CAD/MWh\n",
      "Hour 2 Predispatch RMSE: 29.67 CAD/MWh\n",
      "Hour 2 Predispatch MAE : 11.44 CAD/MWh\n",
      "Hour 3 Predispatch RMSE: 28.38 CAD/MWh\n",
      "Hour 3 Predispatch MAE : 10.54 CAD/MWh\n"
     ]
    }
   ],
   "source": [
    "# Print baseline Metrics from IESO, 2 and 3 hour forecasted RMSE, MAE on 2024 data\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "target = 'HOEP'\n",
    "\n",
    "\n",
    "# 3-way split for proper validation\n",
    "train_start = pd.Timestamp(\"2014-01-01 00:00:00\", tz=\"UTC\")\n",
    "train_cutoff = pd.Timestamp(\"2023-01-01 00:00:00\", tz=\"UTC\")\n",
    "val_cutoff = pd.Timestamp(\"2024-01-01 00:00:00\", tz=\"UTC\")\n",
    "final_cutoff = pd.Timestamp(\"2025-01-01 00:00:00\", tz='UTC')\n",
    "\n",
    "df_train = PDP_df[(PDP_df.index < train_cutoff)] # Train on 2014-2022\n",
    "df_val = PDP_df[(PDP_df.index >= train_cutoff) & (PDP_df.index < val_cutoff)]  # Validate on 2023\n",
    "df_test = PDP_df[(PDP_df.index >= val_cutoff ) & (PDP_df.index < final_cutoff)] # Test on 2024\n",
    "\n",
    "\n",
    "y_test = df_test[target].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Hour 1\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_test, df_test[\"Hour 1 Predispatch\"]))\n",
    "mae_1  = mean_absolute_error(y_test, df_test[\"Hour 1 Predispatch\"])\n",
    "# Hour 2\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_test, df_test[\"Hour 2 Predispatch\"]))\n",
    "mae_2  = mean_absolute_error(y_test, df_test[\"Hour 2 Predispatch\"])\n",
    "\n",
    "# Hour 3\n",
    "rmse_3 = np.sqrt(mean_squared_error(y_test, df_test[\"Hour 3 Predispatch\"]))\n",
    "mae_3  = mean_absolute_error(y_test, df_test[\"Hour 3 Predispatch\"])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Hour 1 Predispatch RMSE: {rmse_1:.2f} CAD/MWh\")\n",
    "print(f\"Hour 1 Predispatch MAE : {mae_1:.2f} CAD/MWh\")\n",
    "print(f\"Hour 2 Predispatch RMSE: {rmse_2:.2f} CAD/MWh\")\n",
    "print(f\"Hour 2 Predispatch MAE : {mae_2:.2f} CAD/MWh\")\n",
    "print(f\"Hour 3 Predispatch RMSE: {rmse_3:.2f} CAD/MWh\")\n",
    "print(f\"Hour 3 Predispatch MAE : {mae_3:.2f} CAD/MWh\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
