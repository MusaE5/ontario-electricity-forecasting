{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b369a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shape: (99300, 5)\n",
      "HOEP                  0\n",
      "Hour 1 Predispatch    0\n",
      "Hour 2 Predispatch    0\n",
      "Hour 3 Predispatch    0\n",
      "Ontario Demand        0\n",
      "dtype: int64\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Read and concat yearly files\n",
    "price_list, demand_list = [], []\n",
    "for year in range(2014, 2026):\n",
    "    p = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_PriceHOEPPredispOR_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    d = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_Demand_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    price_list.append(p); demand_list.append(d)\n",
    "\n",
    "price_df  = pd.concat(price_list,  ignore_index=True)\n",
    "demand_df = pd.concat(demand_list, ignore_index=True)\n",
    "\n",
    "# 2. Clean names\n",
    "price_df.columns  = price_df.columns.str.strip()\n",
    "demand_df.columns = demand_df.columns.str.strip()\n",
    "\n",
    "price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']] = \\\n",
    "    price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']].ffill()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df in (price_df, demand_df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Step 1: Combine Date and Hour into naive datetime\n",
    "    naive_ts = df['Date'] + pd.to_timedelta(df['Hour'] - 1, unit='h')\n",
    "\n",
    "    # Step 2: Localize with correct DST handling\n",
    "    df['timestamp'] = naive_ts.dt.tz_localize(\n",
    "        'Canada/Eastern',\n",
    "        ambiguous=False,               # Handle fall-back DST automatically\n",
    "        nonexistent='shift_forward'   # Handle spring-forward DST\n",
    "    ).dt.tz_convert('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# 4. Drop bad & duplicates\n",
    "price_df  = price_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "demand_df = demand_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "# 5. Merge price + demand on timestamp\n",
    "combined_df = pd.merge(\n",
    "    price_df, demand_df,\n",
    "    on='timestamp', how='inner',\n",
    "    suffixes=('_price','_demand')\n",
    ")\n",
    "combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# 6. Cleanup\n",
    "combined_df = combined_df.drop(\n",
    "    columns=[\n",
    "        'Date_price','Hour_price',\n",
    "        'Date_demand','Hour_demand',\n",
    "        'OR 10 Min Sync','OR 10 Min non-sync','OR 30 Min', 'Market Demand'\n",
    "    ],\n",
    "    errors='ignore'\n",
    ")\n",
    "combined_df = combined_df.set_index('timestamp')\n",
    "\n",
    "\n",
    "print(f\"\\nFinal shape: {combined_df.shape}\")\n",
    "print(combined_df.isna().sum())\n",
    "print(\"Range:\", combined_df.index.min(), \"→\", combined_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ef85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL WEATHER DF ===\n",
      "Shape: (99300, 12)\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Config\n",
    "root = r'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\weather'\n",
    "cities = ['toronto','kitchener','london','ottawa']\n",
    "col_map = {\n",
    "    \"Temp (°C)\":    \"temp\",\n",
    "    \"Rel Hum (%)\":  \"humidity\",\n",
    "    \"Wind Spd (km/h)\": \"wind_speed\",\n",
    "}\n",
    "\n",
    "city_dfs = []\n",
    "for city in cities:\n",
    "    p = os.path.join(root, city)\n",
    "    dfs = []\n",
    "    for fname in sorted(os.listdir(p)):\n",
    "        df = pd.read_csv(os.path.join(p, fname))\n",
    "        df.columns = df.columns.str.strip().str.replace('\"','')\n",
    "\n",
    "        naive_ts = pd.to_datetime(df['Date/Time (LST)'], errors='coerce')\n",
    "\n",
    "        df = df[list(col_map)].rename(columns=col_map)\n",
    "\n",
    "\n",
    "        df['timestamp'] = naive_ts.dt.tz_localize('Canada/Eastern', ambiguous=False, nonexistent='shift_forward').dt.tz_convert('UTC')\n",
    "        dfs.append(df)\n",
    "    \n",
    "   \n",
    "    city_df = pd.concat(dfs, ignore_index=True)\n",
    "    city_df  = city_df.drop_duplicates(subset=['timestamp']) \n",
    "    city_df = city_df.set_index('timestamp').sort_index()\n",
    "    # Now forward fill\n",
    "    city_df = city_df.ffill()\n",
    "    city_df = city_df.add_suffix(f\"_{city}\")\n",
    "    city_dfs.append(city_df)\n",
    "\n",
    "# inner join across all cities (keeps only timestamps present in every city)\n",
    "from functools import reduce\n",
    "weather_df = reduce(lambda L, R: L.join(R, how='inner'), city_dfs)\n",
    "weather_df = weather_df.sort_index()\n",
    "\n",
    "print(\"\\n=== FINAL WEATHER DF ===\")\n",
    "print(\"Shape:\", weather_df.shape)\n",
    "print(\"Range:\", weather_df.index.min(), \"→\", weather_df.index.max())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ No NaNs found in merged data\n",
      "\n",
      "Merged shape: (99300, 17)\n",
      "Merged range: 2014-01-01 05:00:00+00:00 2025-05-01 03:00:00+00:00\n",
      "                            HOEP Hour 1 Predispatch Hour 2 Predispatch  \\\n",
      "timestamp                                                                \n",
      "2014-01-01 05:00:00+00:00  23.04              32.44                 32   \n",
      "2014-01-01 06:00:00+00:00  36.64              32.44              32.44   \n",
      "2014-01-01 07:00:00+00:00  45.85              32.59              32.59   \n",
      "2014-01-01 08:00:00+00:00  30.88              32.44              32.44   \n",
      "2014-01-01 09:00:00+00:00  14.36               32.1               32.1   \n",
      "\n",
      "                          Hour 3 Predispatch  Ontario Demand  temp_toronto  \\\n",
      "timestamp                                                                    \n",
      "2014-01-01 05:00:00+00:00               35.2           15862          -8.3   \n",
      "2014-01-01 06:00:00+00:00              21.59           15462          -8.2   \n",
      "2014-01-01 07:00:00+00:00              32.44           15182          -8.6   \n",
      "2014-01-01 08:00:00+00:00              22.44           14925          -8.8   \n",
      "2014-01-01 09:00:00+00:00               32.1           14807          -8.9   \n",
      "\n",
      "                           humidity_toronto  wind_speed_toronto  \\\n",
      "timestamp                                                         \n",
      "2014-01-01 05:00:00+00:00              77.0                22.0   \n",
      "2014-01-01 06:00:00+00:00              61.0                39.0   \n",
      "2014-01-01 07:00:00+00:00              63.0                30.0   \n",
      "2014-01-01 08:00:00+00:00              69.0                32.0   \n",
      "2014-01-01 09:00:00+00:00              70.0                32.0   \n",
      "\n",
      "                           temp_kitchener  humidity_kitchener  \\\n",
      "timestamp                                                       \n",
      "2014-01-01 05:00:00+00:00           -10.6                69.0   \n",
      "2014-01-01 06:00:00+00:00           -11.2                67.0   \n",
      "2014-01-01 07:00:00+00:00           -11.3                70.0   \n",
      "2014-01-01 08:00:00+00:00           -11.3                72.0   \n",
      "2014-01-01 09:00:00+00:00           -11.2                72.0   \n",
      "\n",
      "                           wind_speed_kitchener  temp_london  humidity_london  \\\n",
      "timestamp                                                                       \n",
      "2014-01-01 05:00:00+00:00                  35.0         -9.2             77.0   \n",
      "2014-01-01 06:00:00+00:00                  28.0         -9.8             71.0   \n",
      "2014-01-01 07:00:00+00:00                  21.0        -10.4             69.0   \n",
      "2014-01-01 08:00:00+00:00                  21.0        -11.1             70.0   \n",
      "2014-01-01 09:00:00+00:00                  21.0        -12.2             72.0   \n",
      "\n",
      "                           wind_speed_london  temp_ottawa  humidity_ottawa  \\\n",
      "timestamp                                                                    \n",
      "2014-01-01 05:00:00+00:00               17.0        -19.3             88.0   \n",
      "2014-01-01 06:00:00+00:00               21.0        -18.7             89.0   \n",
      "2014-01-01 07:00:00+00:00               18.0        -19.9             87.0   \n",
      "2014-01-01 08:00:00+00:00               12.0        -18.7             88.0   \n",
      "2014-01-01 09:00:00+00:00               10.0        -20.4             86.0   \n",
      "\n",
      "                           wind_speed_ottawa  \n",
      "timestamp                                     \n",
      "2014-01-01 05:00:00+00:00                5.0  \n",
      "2014-01-01 06:00:00+00:00                7.0  \n",
      "2014-01-01 07:00:00+00:00                4.0  \n",
      "2014-01-01 08:00:00+00:00                8.0  \n",
      "2014-01-01 09:00:00+00:00                7.0  \n"
     ]
    }
   ],
   "source": [
    "# Both have 'timestamp' index in UTC now, accounts for daylight savings\n",
    "final_df = combined_df.join(weather_df, how='inner')\n",
    "\n",
    "print(\"\\nMerged shape:\", final_df.shape)\n",
    "print(\"Merged range:\", final_df.index.min(), final_df.index.max())\n",
    "print(final_df.isna().sum())\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7696d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'humidity_toronto_ma_{win}'] = df['humidity_toronto_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'wind_speed_toronto_ma_{win}'] = df['wind_speed_toronto_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:67: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'demand_ma_{win}'] = df['demand_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:68: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'HOEP_ma_{win}'] = df['HOEP_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'temp_toronto_ma_{win}'] = df['temp_toronto_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:71: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'humidity_toronto_ma_{win}'] = df['humidity_toronto_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:72: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'wind_speed_toronto_ma_{win}'] = df['wind_speed_toronto_lag_2'].rolling(win).mean()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:74: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['HOEP_volatility_24h'] = df['HOEP_lag_2'].rolling(23).std()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:75: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['HOEP_range_24h'] = df['HOEP_lag_2'].rolling(23).max() - df['HOEP_lag_2'].rolling(23).min()\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_toronto_change_24h'] = df['temp_toronto_lag_2'] - df['temp_toronto_lag_24']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_kitchener_change_24h'] = df['temp_kitchener_lag_2'] - df['temp_kitchener_lag_24']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:80: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_london_change_24h'] = df['temp_london_lag_2'] - df['temp_london_lag_24']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:81: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_ottawa_change_24h'] = df['temp_ottawa_lag_2'] - df['temp_ottawa_lag_24']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_toronto_squared_lag_2'] = df['temp_toronto_lag_2'] ** 2\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_ottawa_squared_lag_2'] = df['temp_ottawa_lag_2'] ** 2\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_kitchener_squared_lag_2'] = df['temp_kitchener_lag_2'] ** 2\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:87: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_london_squared_lag_2'] = df['temp_london_lag_2'] ** 2\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['demand_temp_toronto_interaction'] = df['demand_lag_2'] * df['temp_toronto_lag_2']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:91: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['hour_weekend_interaction'] = df['hour'] * df['is_weekend']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:92: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['temp_humidity_toronto_interaction'] = df['temp_toronto_lag_2'] * df['humidity_toronto_lag_2']\n",
      "C:\\Users\\owner\\AppData\\Local\\Temp\\ipykernel_22112\\1639232999.py:93: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df['demand_HOEP_ratio_lag_2'] = df['demand_lag_2'] / (df['HOEP_lag_2'] + 1e-6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (98953, 123)\n",
      "['timestamp', 'HOEP', 'Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch', 'hour', 'day_of_week', 'month', 'year', 'quarter', 'hour_sin', 'hour_cos', 'is_weekend', 'month_sin', 'month_cos', 'week_of_year', 'week_of_year_sin', 'week_of_year_cos', 'day_of_year', 'doy_sin', 'doy_cos', 'is_holiday', 'is_business_day', 'hour_of_week', 'demand_lag_2', 'HOEP_lag_2', 'temp_toronto_lag_2', 'humidity_toronto_lag_2', 'wind_speed_toronto_lag_2', 'temp_kitchener_lag_2', 'humidity_kitchener_lag_2', 'wind_speed_kitchener_lag_2', 'temp_london_lag_2', 'humidity_london_lag_2', 'wind_speed_london_lag_2', 'temp_ottawa_lag_2', 'humidity_ottawa_lag_2', 'wind_speed_ottawa_lag_2', 'demand_lag_3', 'HOEP_lag_3', 'temp_toronto_lag_3', 'humidity_toronto_lag_3', 'wind_speed_toronto_lag_3', 'temp_kitchener_lag_3', 'humidity_kitchener_lag_3', 'wind_speed_kitchener_lag_3', 'temp_london_lag_3', 'humidity_london_lag_3', 'wind_speed_london_lag_3', 'temp_ottawa_lag_3', 'humidity_ottawa_lag_3', 'wind_speed_ottawa_lag_3', 'demand_lag_24', 'HOEP_lag_24', 'temp_toronto_lag_24', 'humidity_toronto_lag_24', 'wind_speed_toronto_lag_24', 'temp_kitchener_lag_24', 'humidity_kitchener_lag_24', 'wind_speed_kitchener_lag_24', 'temp_london_lag_24', 'humidity_london_lag_24', 'wind_speed_london_lag_24', 'temp_ottawa_lag_24', 'humidity_ottawa_lag_24', 'wind_speed_ottawa_lag_24', 'demand_lag_48', 'HOEP_lag_48', 'temp_toronto_lag_48', 'humidity_toronto_lag_48', 'wind_speed_toronto_lag_48', 'temp_kitchener_lag_48', 'humidity_kitchener_lag_48', 'wind_speed_kitchener_lag_48', 'temp_london_lag_48', 'humidity_london_lag_48', 'wind_speed_london_lag_48', 'temp_ottawa_lag_48', 'humidity_ottawa_lag_48', 'wind_speed_ottawa_lag_48', 'demand_lag_168', 'HOEP_lag_168', 'temp_toronto_lag_168', 'humidity_toronto_lag_168', 'wind_speed_toronto_lag_168', 'temp_kitchener_lag_168', 'humidity_kitchener_lag_168', 'wind_speed_kitchener_lag_168', 'temp_london_lag_168', 'humidity_london_lag_168', 'wind_speed_london_lag_168', 'temp_ottawa_lag_168', 'humidity_ottawa_lag_168', 'wind_speed_ottawa_lag_168', 'demand_ma_3', 'HOEP_ma_3', 'temp_toronto_ma_3', 'humidity_toronto_ma_3', 'wind_speed_toronto_ma_3', 'demand_ma_23', 'HOEP_ma_23', 'temp_toronto_ma_23', 'humidity_toronto_ma_23', 'wind_speed_toronto_ma_23', 'demand_ma_167', 'HOEP_ma_167', 'temp_toronto_ma_167', 'humidity_toronto_ma_167', 'wind_speed_toronto_ma_167', 'HOEP_volatility_24h', 'HOEP_range_24h', 'temp_toronto_change_24h', 'temp_kitchener_change_24h', 'temp_london_change_24h', 'temp_ottawa_change_24h', 'temp_toronto_squared_lag_2', 'temp_ottawa_squared_lag_2', 'temp_kitchener_squared_lag_2', 'temp_london_squared_lag_2', 'demand_temp_toronto_interaction', 'hour_weekend_interaction', 'temp_humidity_toronto_interaction', 'demand_HOEP_ratio_lag_2']\n",
      "\n",
      "Feature counts:\n",
      "  Temporal:   18\n",
      "  Lag:        75\n",
      "  Rolling:    17\n",
      "  Interaction:4\n",
      "  Weather:    8\n",
      "  Total:      114\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "lags = [2, 3, 24, 48, 168]\n",
    "roll_windows = [3, 23, 167]\n",
    "ontario_holidays = holidays.Canada(prov='ON')\n",
    "\n",
    "# ---------------- LOAD + HOURLY ALIGNMENT ----------------\n",
    "df = final_df.copy()\n",
    "df = df.reset_index()\n",
    "# Convert HOEP from string to numeric\n",
    "df['HOEP'] = pd.to_numeric(df['HOEP'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 1 Predispatch'] = pd.to_numeric(df['Hour 1 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 2 Predispatch'] = pd.to_numeric(df['Hour 2 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 3 Predispatch'] = pd.to_numeric(df['Hour 3 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# ---------------- TIME FEATURES ----------------\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['quarter'] = df['timestamp'].dt.quarter\n",
    "\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['week_of_year'] = df['timestamp'].dt.isocalendar().week\n",
    "df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['is_holiday'] = df['timestamp'].dt.date.isin(ontario_holidays).astype(int)\n",
    "df['is_business_day'] = ((df['day_of_week'] < 5) & (~df['is_holiday'])).astype(int)\n",
    "df['hour_of_week'] = df['day_of_week'] * 24 + df['hour']\n",
    "\n",
    "# ---------------- LAG FEATURES ----------------\n",
    "for k in lags:\n",
    "    df[f'demand_lag_{k}'] = df['Ontario Demand'].shift(k)\n",
    "    df[f'HOEP_lag_{k}'] = df['HOEP'].shift(k)\n",
    "\n",
    "    \n",
    "    # Add lags for each city\n",
    "    df[f'temp_toronto_lag_{k}'] = df['temp_toronto'].shift(k)\n",
    "    df[f'humidity_toronto_lag_{k}'] = df['humidity_toronto'].shift(k)\n",
    "    df[f'wind_speed_toronto_lag_{k}'] = df['wind_speed_toronto'].shift(k)\n",
    "    \n",
    "    df[f'temp_kitchener_lag_{k}'] = df['temp_kitchener'].shift(k)\n",
    "    df[f'humidity_kitchener_lag_{k}'] = df['humidity_kitchener'].shift(k)\n",
    "    df[f'wind_speed_kitchener_lag_{k}'] = df['wind_speed_kitchener'].shift(k)\n",
    "    \n",
    "    df[f'temp_london_lag_{k}'] = df['temp_london'].shift(k)\n",
    "    df[f'humidity_london_lag_{k}'] = df['humidity_london'].shift(k)\n",
    "    df[f'wind_speed_london_lag_{k}'] = df['wind_speed_london'].shift(k)\n",
    "    \n",
    "    df[f'temp_ottawa_lag_{k}'] = df['temp_ottawa'].shift(k)\n",
    "    df[f'humidity_ottawa_lag_{k}'] = df['humidity_ottawa'].shift(k)\n",
    "    df[f'wind_speed_ottawa_lag_{k}'] = df['wind_speed_ottawa'].shift(k)\n",
    "\n",
    "\n",
    "for win in roll_windows:\n",
    "    df[f'demand_ma_{win}'] = df['demand_lag_2'].rolling(win).mean()\n",
    "    df[f'HOEP_ma_{win}'] = df['HOEP_lag_2'].rolling(win).mean()\n",
    "    \n",
    "    df[f'temp_toronto_ma_{win}'] = df['temp_toronto_lag_2'].rolling(win).mean()\n",
    "    df[f'humidity_toronto_ma_{win}'] = df['humidity_toronto_lag_2'].rolling(win).mean()\n",
    "    df[f'wind_speed_toronto_ma_{win}'] = df['wind_speed_toronto_lag_2'].rolling(win).mean()\n",
    "\n",
    "df['HOEP_volatility_24h'] = df['HOEP_lag_2'].rolling(23).std()\n",
    "df['HOEP_range_24h'] = df['HOEP_lag_2'].rolling(23).max() - df['HOEP_lag_2'].rolling(23).min()\n",
    "\n",
    "# Temperature changes for each city\n",
    "df['temp_toronto_change_24h'] = df['temp_toronto_lag_2'] - df['temp_toronto_lag_24']\n",
    "df['temp_kitchener_change_24h'] = df['temp_kitchener_lag_2'] - df['temp_kitchener_lag_24']\n",
    "df['temp_london_change_24h'] = df['temp_london_lag_2'] - df['temp_london_lag_24']\n",
    "df['temp_ottawa_change_24h'] = df['temp_ottawa_lag_2'] - df['temp_ottawa_lag_24']\n",
    "\n",
    "\n",
    "df['temp_toronto_squared_lag_2'] = df['temp_toronto_lag_2'] ** 2\n",
    "df['temp_ottawa_squared_lag_2'] = df['temp_ottawa_lag_2'] ** 2\n",
    "df['temp_kitchener_squared_lag_2'] = df['temp_kitchener_lag_2'] ** 2\n",
    "df['temp_london_squared_lag_2'] = df['temp_london_lag_2'] ** 2\n",
    "\n",
    "# ---------------- INTERACTIONS ----------------\n",
    "df['demand_temp_toronto_interaction'] = df['demand_lag_2'] * df['temp_toronto_lag_2']\n",
    "df['hour_weekend_interaction'] = df['hour'] * df['is_weekend']\n",
    "df['temp_humidity_toronto_interaction'] = df['temp_toronto_lag_2'] * df['humidity_toronto_lag_2']\n",
    "df['demand_HOEP_ratio_lag_2'] = df['demand_lag_2'] / (df['HOEP_lag_2'] + 1e-6)\n",
    "\n",
    "# ---------------- CLEANUP ----------------\n",
    "valid_rows_start = 168 + max(lags)\n",
    "df = df.iloc[valid_rows_start:]\n",
    "df = df.dropna()\n",
    "\n",
    "# ---------------- REPORT ----------------\n",
    "temporal_features = ['hour', 'day_of_week', 'month', 'year', 'quarter',\n",
    "                    'hour_sin', 'hour_cos', 'is_weekend', 'month_sin', 'month_cos', \n",
    "                    'week_of_year_sin', 'week_of_year_cos', 'day_of_year', 'doy_sin', 'doy_cos',\n",
    "                    'is_holiday', 'is_business_day', 'hour_of_week']\n",
    "lag_features = [col for col in df.columns if '_lag_' in col]\n",
    "rolling_features = [col for col in df.columns if '_ma_' in col or 'volatility' in col or 'range' in col]\n",
    "interaction_features = [col for col in df.columns if 'interaction' in col or 'ratio' in col]\n",
    "weather_derived = [col for col in df.columns if 'change_24h' in col or 'squared_lag_2' in col]\n",
    "\n",
    "df = df.drop(columns=[\n",
    "        'Ontario Demand',   'temp_toronto', 'temp_london', 'temp_ottawa', 'temp_kitchener',\n",
    "    'wind_speed_toronto', 'wind_speed_london', 'wind_speed_ottawa', 'wind_speed_kitchener',\n",
    "    'humidity_toronto', 'humidity_london', 'humidity_ottawa', 'humidity_kitchener'\n",
    "    ])\n",
    "df = df.dropna()\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "features = temporal_features + lag_features + rolling_features + interaction_features \n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Temporal:   {len(temporal_features)}\")\n",
    "print(f\"  Lag:        {len(lag_features)}\")\n",
    "print(f\"  Rolling:    {len(rolling_features)}\")\n",
    "print(f\"  Interaction:{len(interaction_features)}\")\n",
    "print(f\"  Weather:    {len(weather_derived)}\")\n",
    "print(f\"  Total:      {len(features)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f0c69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib as plt \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Define quantile loss function\n",
    "def quantile_loss(q):\n",
    "    \"\"\"Custom loss function for quantile regression\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        return tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n",
    "    return loss\n",
    "\n",
    "# Define multiple quantiles to predict\n",
    "quantiles = [0.1, 0.5, 0.9]  # 10th, 50th (median), 90th percentiles\n",
    "\n",
    "# Method 1: Single model predicting multiple quantiles\n",
    "def create_multi_quantile_model(input_shape, quantiles):\n",
    "    \"\"\"Create model that predicts multiple quantiles simultaneously\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(len(quantiles), activation='linear')  # One output per quantile\n",
    "    ])\n",
    "    \n",
    "    # Custom combined loss for multiple quantiles\n",
    "    def combined_quantile_loss(y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i, q in enumerate(quantiles):\n",
    "            q_pred = y_pred[:, i:i+1]\n",
    "            error = y_true - q_pred\n",
    "            total_loss += tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n",
    "        return total_loss\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.001), loss=combined_quantile_loss)\n",
    "    return model\n",
    "\n",
    "# Method 2: Separate models for each quantile \n",
    "def create_single_quantile_model(input_shape, quantile):\n",
    "    \"\"\"Create model for single quantile prediction\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.001), loss=quantile_loss(quantile))\n",
    "    return model\n",
    "\n",
    "# Training function for quantile regression\n",
    "def train_quantile_models(X_train, y_train, X_test, y_test, method='separate'):\n",
    "    \"\"\"Train quantile regression models\"\"\"\n",
    "    \n",
    "    if method == 'combined':\n",
    "        # Method 1: Single model for all quantiles\n",
    "        model = create_multi_quantile_model(X_train.shape[1], quantiles)\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Predict all quantiles\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "        \n",
    "        # Separate predictions by quantile\n",
    "        quantile_predictions = {}\n",
    "        for i, q in enumerate(quantiles):\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = predictions[:, i]\n",
    "            \n",
    "    else:\n",
    "        # Method 2: Separate models for each quantile (recommended)\n",
    "        quantile_models = {}\n",
    "        quantile_predictions = {}\n",
    "        \n",
    "        for q in quantiles:\n",
    "            print(f\"Training quantile {q} model...\")\n",
    "            \n",
    "            model = create_single_quantile_model(X_train.shape[1], q)\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.1,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Store model and predictions\n",
    "            quantile_models[f'q_{int(q*100)}'] = model\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    return quantile_predictions\n",
    "\n",
    "# Evaluation metrics for quantile regression\n",
    "def evaluate_quantile_predictions(y_true, quantile_predictions):\n",
    "    \"\"\"Evaluate quantile regression performance\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for q_name, q_pred in quantile_predictions.items():\n",
    "        q_value = float(q_name.split('_')[1]) / 100\n",
    "        \n",
    "        # Pinball loss (quantile loss)\n",
    "        error = y_true - q_pred\n",
    "        pinball_loss = np.mean(np.maximum(q_value * error, (q_value - 1) * error))\n",
    "        \n",
    "        # Coverage (what % of actual values fall below this quantile)\n",
    "        coverage = np.mean(y_true <= q_pred)\n",
    "        \n",
    "        # RMSE for this quantile\n",
    "        rmse = np.sqrt(np.mean((y_true - q_pred)**2))\n",
    "        \n",
    "        results[q_name] = {\n",
    "            'pinball_loss': pinball_loss,\n",
    "            'coverage': coverage,\n",
    "            'expected_coverage': q_value,\n",
    "            'rmse': rmse\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prediction interval metrics\n",
    "def calculate_prediction_intervals(quantile_predictions, confidence_levels=[0.8]):\n",
    "    \"\"\"Calculate prediction intervals from quantile predictions\"\"\"\n",
    "    intervals = {}\n",
    "    \n",
    "    for conf_level in confidence_levels:\n",
    "        alpha = 1 - conf_level\n",
    "        lower_q = f\"q_{int((alpha/2)*100)}\"\n",
    "        upper_q = f\"q_{int((1-alpha/2)*100)}\"\n",
    "        \n",
    "        if lower_q in quantile_predictions and upper_q in quantile_predictions:\n",
    "            intervals[f'{int(conf_level*100)}%'] = {\n",
    "                'lower': quantile_predictions[lower_q],\n",
    "                'upper': quantile_predictions[upper_q],\n",
    "                'width': quantile_predictions[upper_q] - quantile_predictions[lower_q]\n",
    "            }\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "# Visualization function\n",
    "def plot_quantile_predictions(y_true, quantile_predictions, start_idx=0, end_idx=200):\n",
    "    \"\"\"Plot quantile predictions with uncertainty bands\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual values\n",
    "    x = range(start_idx, min(end_idx, len(y_true)))\n",
    "    plt.plot(x, y_true[start_idx:end_idx], 'k-', label='Actual HOEP', linewidth=2)\n",
    "    \n",
    "    # Plot median prediction\n",
    "    if 'q_50' in quantile_predictions:\n",
    "        plt.plot(x, quantile_predictions['q_50'][start_idx:end_idx], 'r--', \n",
    "                label='Median Prediction', linewidth=2)\n",
    "    \n",
    "    # Plot uncertainty bands\n",
    "    if 'q_10' in quantile_predictions and 'q_90' in quantile_predictions:\n",
    "        plt.fill_between(x, \n",
    "                        quantile_predictions['q_10'][start_idx:end_idx],\n",
    "                        quantile_predictions['q_90'][start_idx:end_idx],\n",
    "                        alpha=0.3, color='blue', label='80% Prediction Interval')\n",
    "    \n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('HOEP (CAD/MWh)')\n",
    "    plt.title('Quantile Regression Predictions with Uncertainty')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4058d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time based, no Train test split\n",
    "df_train = df[df[\"timestamp\"] < \"2024-01-01\"]\n",
    "df_test  = df[df[\"timestamp\"] >= \"2024-01-01\"]\n",
    "\n",
    "target = \"HOEP\"\n",
    "\n",
    "\n",
    "X_train_raw = df_train[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_train     = pd.to_numeric(df_train[target], errors=\"coerce\")\n",
    "X_test_raw  = df_test[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_test      = pd.to_numeric(df_test[target], errors=\"coerce\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9d8806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quantile regression models...\n",
      "Training quantile 0.1 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quantile 0.5 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training quantile 0.9 model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating quantile predictions...\n",
      "\n",
      "=== Quantile Regression Results ===\n",
      "\n",
      "Quantile q_10:\n",
      "  RMSE: 38.03\n",
      "  Pinball Loss: 2.00\n",
      "  Coverage: 0.095 (expected: 0.100)\n",
      "\n",
      "Quantile q_50:\n",
      "  RMSE: 33.96\n",
      "  Pinball Loss: 5.22\n",
      "  Coverage: 0.417 (expected: 0.500)\n",
      "\n",
      "Quantile q_90:\n",
      "  RMSE: 38.11\n",
      "  Pinball Loss: 4.54\n",
      "  Coverage: 0.890 (expected: 0.900)\n",
      "\n",
      "=== Prediction Intervals ===\n",
      "Median quantile RMSE: 33.96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "print(\"Training quantile regression models...\")\n",
    "quantile_predictions = train_quantile_models(X_train, y_train, X_test, y_test, method='separate')\n",
    "    \n",
    "print(\"\\nEvaluating quantile predictions...\")\n",
    "results = evaluate_quantile_predictions(y_test, quantile_predictions)\n",
    "    \n",
    "print(\"\\n=== Quantile Regression Results ===\")\n",
    "for q_name, metrics in results.items():\n",
    "    print(f\"\\nQuantile {q_name}:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"  Pinball Loss: {metrics['pinball_loss']:.2f}\")\n",
    "    print(f\"  Coverage: {metrics['coverage']:.3f} (expected: {metrics['expected_coverage']:.3f})\")\n",
    "    \n",
    "# Calculate prediction intervals\n",
    "intervals = calculate_prediction_intervals(quantile_predictions)\n",
    "    \n",
    "print(\"\\n=== Prediction Intervals ===\")\n",
    "for interval_name, interval_data in intervals.items():\n",
    "    avg_width = np.mean(interval_data['width'])\n",
    "    print(f\"{interval_name} interval average width: {avg_width:.2f} CAD/MWh\")\n",
    "\n",
    "    \n",
    "print(f\"Median quantile RMSE: {results['q_50']['rmse']:.2f}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30b31a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ELM RMSE: 33.86\n"
     ]
    }
   ],
   "source": [
    "# Basic ELM\n",
    "np.random.seed(47)\n",
    "n_hidden = 750\n",
    "\n",
    "# Random weights (never trained)\n",
    "W_in = np.random.randn(X_train.shape[1], n_hidden)\n",
    "\n",
    "# Hidden layer\n",
    "H_train = np.maximum(0, X_train @ W_in)\n",
    "H_test = np.maximum(0, X_test @ W_in)\n",
    "\n",
    "# Output weights via Moore-Penrose inverse\n",
    "W_out = np.linalg.pinv(H_train) @ y_train\n",
    "\n",
    "# Predictions\n",
    "y_pred = H_test @ W_out\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "print(f\"ELM RMSE: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae59be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "def create_shallow_transformer(input_dim, \n",
    "                              d_model=64,\n",
    "                              num_heads=4, \n",
    "                              ff_dim=128,\n",
    "                              dropout_rate=0.1):\n",
    "    \"\"\"\n",
    "    Shallow transformer for tabular data regression\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    \n",
    "    # Project to d_model dimensions\n",
    "    x = layers.Dense(d_model)(inputs)\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Add \"sequence\" dimension (needed for attention)\n",
    "    x = layers.Reshape((1, d_model))(x)  # Shape: (batch, 1, d_model)\n",
    "    \n",
    "    # Single Transformer block\n",
    "    # Multi-head attention\n",
    "    attn_output = layers.MultiHeadAttention(\n",
    "        num_heads=num_heads, \n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate\n",
    "    )(x, x)\n",
    "    x = layers.Add()([x, attn_output])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Feed forward\n",
    "    ffn = keras.Sequential([\n",
    "        layers.Dense(ff_dim, activation=\"relu\"),\n",
    "        layers.Dropout(dropout_rate),\n",
    "        layers.Dense(d_model),\n",
    "        layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    ffn_output = ffn(x)\n",
    "    x = layers.Add()([x, ffn_output])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    # Remove sequence dimension and project to output\n",
    "    x = layers.Reshape((d_model,))(x)  # Flatten back\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"Training Shallow Transformer...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Create model\n",
    "model = create_shallow_transformer(\n",
    "    input_dim=X_train.shape[1],\n",
    "    d_model=64,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "# Train with early stopping\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test, verbose=0).flatten()\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"Training time: {train_time:.1f} seconds\")\n",
    "print(f\"Transformer RMSE: {rmse:.2f}\")\n",
    "print(f\"Your NN RMSE: 33.74\")\n",
    "print(f\"Difference: {rmse - 33.74:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e68e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No CSV available for PUB_Supply on 20240719 (likely 404 or HTML page)\n",
      "❌ No CSV available for PUB_IntertieSchedule on 20240719 (likely 404 or HTML page)\n",
      "❌ No CSV available for PUB_Supply on 20240718 (likely 404 or HTML page)\n",
      "❌ No CSV available for PUB_IntertieSchedule on 20240718 (likely 404 or HTML page)\n",
      "❌ No CSV available for PUB_Supply on 20240717 (likely 404 or HTML page)\n",
      "❌ No CSV available for PUB_IntertieSchedule on 20240717 (likely 404 or HTML page)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def try_ieso_csv(report_type, date_str):\n",
    "    url = f\"https://reports.ieso.ca/public/{report_type}/{report_type}_{date_str}.csv\"\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if \"html\" in response.headers.get(\"Content-Type\", \"\"):\n",
    "        print(f\"❌ No CSV available for {report_type} on {date_str} (likely 404 or HTML page)\")\n",
    "        return None\n",
    "\n",
    "    raw = response.text\n",
    "    for delim in [\",\", \";\", \"\\t\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(StringIO(raw), delimiter=delim, skip_blank_lines=True)\n",
    "            if df.shape[1] > 1 and df.shape[0] > 10:\n",
    "                print(f\"\\n✅ Parsed {report_type}_{date_str}.csv with delimiter '{delim}'\")\n",
    "                print(f\"Columns: {df.columns.tolist()}\")\n",
    "                print(df.head(3))\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    print(f\"❌ Failed to parse {report_type}_{date_str}.csv — invalid format.\")\n",
    "    return None\n",
    "\n",
    "# Try known working days — IESO typically has reports Mon–Fri\n",
    "for date_str in [\"20240719\", \"20240718\", \"20240717\"]:  # Weekdays\n",
    "    try_ieso_csv(\"PUB_Supply\", date_str)\n",
    "    try_ieso_csv(\"PUB_IntertieSchedule\", date_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
