{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b369a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shape: (99300, 5)\n",
      "HOEP                  0\n",
      "Hour 1 Predispatch    0\n",
      "Hour 2 Predispatch    0\n",
      "Hour 3 Predispatch    0\n",
      "Ontario Demand        0\n",
      "dtype: int64\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read and concat yearly files\n",
    "price_list, demand_list = [], []\n",
    "for year in range(2014, 2026):\n",
    "    p = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_PriceHOEPPredispOR_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    d = pd.read_csv(\n",
    "        fr'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\PUB_Demand_{year}.csv',\n",
    "        skiprows=3\n",
    "    )\n",
    "    price_list.append(p); demand_list.append(d)\n",
    "\n",
    "price_df  = pd.concat(price_list,  ignore_index=True)\n",
    "demand_df = pd.concat(demand_list, ignore_index=True)\n",
    "\n",
    "# Clean names\n",
    "price_df.columns  = price_df.columns.str.strip()\n",
    "demand_df.columns = demand_df.columns.str.strip()\n",
    "\n",
    "price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']] = \\\n",
    "    price_df[['Hour 1 Predispatch', 'Hour 2 Predispatch', 'Hour 3 Predispatch']].ffill() # We use forward fill for NaNs as they appear in <1 percent of the data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for df in (price_df, demand_df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "    # Combine Date and Hour into naive datetime\n",
    "    naive_ts = df['Date'] + pd.to_timedelta(df['Hour'] - 1, unit='h')\n",
    "\n",
    "    # Localize with correct DST handling\n",
    "    df['timestamp'] = naive_ts.dt.tz_localize(\n",
    "        'Canada/Eastern',\n",
    "        ambiguous=False,               # Convert to UTC to handle day light saving shifts in dataFrame\n",
    "        nonexistent='shift_forward'   \n",
    "    ).dt.tz_convert('UTC')\n",
    "\n",
    "\n",
    "\n",
    "# Drop duplicates from daylight savings\n",
    "price_df  = price_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "demand_df = demand_df.dropna(subset=['timestamp']).drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "# Merge price + demand on timestamp\n",
    "combined_df = pd.merge(\n",
    "    price_df, demand_df,\n",
    "    on='timestamp', how='inner',\n",
    "    suffixes=('_price','_demand')\n",
    ")\n",
    "combined_df = combined_df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# 6. Cleanup\n",
    "combined_df = combined_df.drop(  # Drop non-live compatible features and duplicates columns\n",
    "    columns=[\n",
    "        'Date_price','Hour_price',\n",
    "        'Date_demand','Hour_demand',\n",
    "        'OR 10 Min Sync','OR 10 Min non-sync','OR 30 Min', 'Market Demand'\n",
    "    ],\n",
    "    errors='ignore'\n",
    ")\n",
    "combined_df = combined_df.set_index('timestamp')\n",
    "\n",
    "\n",
    "print(f\"\\nFinal shape: {combined_df.shape}\")\n",
    "print(combined_df.isna().sum())\n",
    "print(\"Range:\", combined_df.index.min(), \"→\", combined_df.index.max()) # Final output (number of NaNs for each column, range, and shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40ef85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (99300, 12)\n",
      "Range: 2014-01-01 05:00:00+00:00 → 2025-05-01 03:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load weather folder paths\n",
    "root = r'C:\\Users\\owner\\Documents\\Projects\\hoep_forecasting_app\\data\\raw\\weather'\n",
    "cities = ['toronto','kitchener','london','ottawa']\n",
    "col_map = {\n",
    "    \"Temp (°C)\":    \"temp\",\n",
    "    \"Rel Hum (%)\":  \"humidity\",\n",
    "    \"Wind Spd (km/h)\": \"wind_speed\",\n",
    "}\n",
    "\n",
    "city_dfs = []\n",
    "for city in cities:\n",
    "    path = os.path.join(root, city) # Becomes raw/weather/toronto , kitchener etc..\n",
    "    dfs = []\n",
    "    for fname in sorted(os.listdir(path)):  # Sort from 2014-2025\n",
    "        df = pd.read_csv(os.path.join(path, fname))\n",
    "        df.columns = df.columns.str.strip().str.replace('\"','') # Clean columns\n",
    "\n",
    "        naive_ts = pd.to_datetime(df['Date/Time (LST)'], errors='coerce')\n",
    "\n",
    "        df = df[list(col_map)].rename(columns=col_map) # Rename wanted columns and drop unwanted ones\n",
    "\n",
    "\n",
    "        df['timestamp'] = naive_ts.dt.tz_localize('Canada/Eastern', ambiguous=False, nonexistent='shift_forward').dt.tz_convert('UTC') # UTC timestamp \n",
    "        dfs.append(df)\n",
    "    \n",
    "   \n",
    "    city_df = pd.concat(dfs, ignore_index=True)\n",
    "    city_df  = city_df.drop_duplicates(subset=['timestamp']) # Drop daylight saving timestamps, we use shift forward to create duplicates and then drop them\n",
    "    city_df = city_df.set_index('timestamp').sort_index()\n",
    "    \n",
    "    city_df = city_df.ffill() # We use forward fill as close to 1 percent data missing. Ffil is sufficent\n",
    "    city_df = city_df.add_suffix(f\"_{city}\")\n",
    "    city_dfs.append(city_df) # Contains each cities sorted cleaned and filled df\n",
    "\n",
    "# inner join across all cities (keeps only timestamps present in every city)\n",
    "from functools import reduce\n",
    "weather_df = reduce(lambda L, R: L.join(R, how='inner'), city_dfs)\n",
    "weather_df = weather_df.sort_index()\n",
    "\n",
    "print(\"Shape:\", weather_df.shape)\n",
    "print(\"Range:\", weather_df.index.min(), \"→\", weather_df.index.max())  # Shape and ranges matches previous cell df\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af0927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged shape: (99300, 17)\n",
      "Merged range: 2014-01-01 05:00:00+00:00 2025-05-01 03:00:00+00:00\n",
      "HOEP                    0\n",
      "Hour 2 Predispatch      0\n",
      "Hour 3 Predispatch      0\n",
      "Ontario Demand          0\n",
      "temp_toronto            0\n",
      "humidity_toronto        0\n",
      "wind_speed_toronto      0\n",
      "temp_kitchener          0\n",
      "humidity_kitchener      0\n",
      "wind_speed_kitchener    0\n",
      "temp_london             0\n",
      "humidity_london         0\n",
      "wind_speed_london       0\n",
      "temp_ottawa             0\n",
      "humidity_ottawa         0\n",
      "wind_speed_ottawa       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Both have 'timestamp' index in UTC now, accounts for daylight savings\n",
    "final_df = combined_df.join(weather_df, how='inner')\n",
    "\n",
    "print(\"\\nMerged shape:\", final_df.shape)\n",
    "print(\"Merged range:\", final_df.index.min(), final_df.index.max())\n",
    "\n",
    "final_df = final_df.drop(columns=['Hour 1 Predispatch'])\n",
    "print(final_df.isna().sum())\n",
    "# Final Nan check , range and shape check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b7696d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shape: (99132, 113)\n",
      "Number of features: 112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import holidays\n",
    "\n",
    "# Config\n",
    "lags = [2, 3, 24, 48, 168]\n",
    "roll_windows = [3, 23, 167]\n",
    "ontario_holidays = holidays.Canada(prov='ON')\n",
    "\n",
    "# Reset index to create time features using timestamp\n",
    "df = final_df.copy()\n",
    "df = df.reset_index()\n",
    "\n",
    "# Convert from string to numeric\n",
    "df['HOEP'] = pd.to_numeric(df['HOEP'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 2 Predispatch'] = pd.to_numeric(df['Hour 2 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df['Hour 3 Predispatch'] = pd.to_numeric(df['Hour 3 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "# Change to datetime object for feature engineering\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Time features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['quarter'] = df['timestamp'].dt.quarter\n",
    "\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['week_of_year'] = df['timestamp'].dt.isocalendar().week\n",
    "df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['is_holiday'] = df['timestamp'].dt.date.isin(ontario_holidays).astype(int)\n",
    "df['is_business_day'] = ((df['day_of_week'] < 5) & (~df['is_holiday'])).astype(int)\n",
    "df['hour_of_week'] = df['day_of_week'] * 24 + df['hour']\n",
    "\n",
    "# Lag features (2,3,24,48 168)\n",
    "lag_features = {}\n",
    "for k in lags:\n",
    "    lag_features[f'demand_lag_{k}'] = df['Ontario Demand'].shift(k)\n",
    "    lag_features[f'HOEP_lag_{k}'] = df['HOEP'].shift(k)\n",
    "\n",
    "    lag_features[f'temp_toronto_lag_{k}'] = df['temp_toronto'].shift(k)\n",
    "    lag_features[f'humidity_toronto_lag_{k}'] = df['humidity_toronto'].shift(k)\n",
    "    lag_features[f'wind_speed_toronto_lag_{k}'] = df['wind_speed_toronto'].shift(k)\n",
    "\n",
    "    lag_features[f'temp_kitchener_lag_{k}'] = df['temp_kitchener'].shift(k)\n",
    "    lag_features[f'humidity_kitchener_lag_{k}'] = df['humidity_kitchener'].shift(k)\n",
    "    lag_features[f'wind_speed_kitchener_lag_{k}'] = df['wind_speed_kitchener'].shift(k)\n",
    "\n",
    "    lag_features[f'temp_london_lag_{k}'] = df['temp_london'].shift(k)\n",
    "    lag_features[f'humidity_london_lag_{k}'] = df['humidity_london'].shift(k)\n",
    "    lag_features[f'wind_speed_london_lag_{k}'] = df['wind_speed_london'].shift(k)\n",
    "\n",
    "    lag_features[f'temp_ottawa_lag_{k}'] = df['temp_ottawa'].shift(k)\n",
    "    lag_features[f'humidity_ottawa_lag_{k}'] = df['humidity_ottawa'].shift(k)\n",
    "    lag_features[f'wind_speed_ottawa_lag_{k}'] = df['wind_speed_ottawa'].shift(k)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(lag_features, index=df.index)], axis=1)\n",
    "\n",
    "# Rolling features\n",
    "roll_features = {}\n",
    "for win in roll_windows:\n",
    "    roll_features[f'demand_ma_{win}'] = df['demand_lag_2'].rolling(win).mean()\n",
    "    roll_features[f'HOEP_ma_{win}'] = df['HOEP_lag_2'].rolling(win).mean()\n",
    "    roll_features[f'temp_toronto_ma_{win}'] = df['temp_toronto_lag_2'].rolling(win).mean()\n",
    "    roll_features[f'humidity_toronto_ma_{win}'] = df['humidity_toronto_lag_2'].rolling(win).mean()\n",
    "    roll_features[f'wind_speed_toronto_ma_{win}'] = df['wind_speed_toronto_lag_2'].rolling(win).mean()\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(roll_features, index=df.index)], axis=1)\n",
    "\n",
    "# Volitality and range\n",
    "df['HOEP_volatility_24h'] = df['HOEP_lag_2'].rolling(23).std()\n",
    "df['HOEP_range_24h'] = df['HOEP_lag_2'].rolling(23).max() - df['HOEP_lag_2'].rolling(23).min()\n",
    "\n",
    "# Squared temp features\n",
    "squared_features = {\n",
    "    'temp_toronto_squared_lag_2': df['temp_toronto_lag_2'] ** 2,\n",
    "    'temp_ottawa_squared_lag_2': df['temp_ottawa_lag_2'] ** 2,\n",
    "    'temp_kitchener_squared_lag_2': df['temp_kitchener_lag_2'] ** 2,\n",
    "    'temp_london_squared_lag_2': df['temp_london_lag_2'] ** 2,\n",
    "}\n",
    "df = pd.concat([df, pd.DataFrame(squared_features, index=df.index)], axis=1)\n",
    "\n",
    "# Final cleanup\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "df = df.set_index('timestamp').sort_index()\n",
    "df = df.dropna()\n",
    "\n",
    "# Drop known leakage columns\n",
    "cols_to_drop = [\n",
    "    'Ontario Demand',\n",
    "    'temp_toronto', 'temp_kitchener', 'temp_ottawa', 'temp_london',\n",
    "    'humidity_toronto', 'humidity_kitchener', 'humidity_ottawa', 'humidity_london',\n",
    "    'wind_speed_toronto', 'wind_speed_kitchener', 'wind_speed_ottawa', 'wind_speed_london'\n",
    "]\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"Final shape: {df.shape}\")\n",
    "\n",
    "\n",
    "# Collect features\n",
    "target_column = 'HOEP'\n",
    "all_features = df.columns.difference([target_column]).tolist()\n",
    "features = list(dict.fromkeys(all_features))  # Every feature except HOEP (target)\n",
    "\n",
    "print(f\"Number of features: {len(features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4058d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-way split for proper validation\n",
    "train_cutoff = pd.Timestamp(\"2023-01-01 00:00:00\", tz=\"UTC\")\n",
    "val_cutoff = pd.Timestamp(\"2024-01-01 00:00:00\", tz=\"UTC\")\n",
    "final_cutoff = pd.Timestamp(\"2025-01-01 00:00:00\", tz='UTC')\n",
    "\n",
    "df_train = df[df.index < train_cutoff]          # 2014-2022\n",
    "df_val = df[(df.index >= train_cutoff) & (df.index < val_cutoff)]  # 2023\n",
    "df_test = df[(df.index >= val_cutoff ) & (df.index < final_cutoff)]             # 2024\n",
    "\n",
    "\n",
    "X_train = df_train[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_train = pd.to_numeric(df_train[target], errors=\"coerce\")\n",
    "\n",
    "X_val = df_val[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_val = pd.to_numeric(df_val[target], errors=\"coerce\")\n",
    "\n",
    "X_test = df_test[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_test = pd.to_numeric(df_test[target], errors=\"coerce\")\n",
    "\n",
    "# Save column names before scaling\n",
    "feature_names_ELM = X_train.columns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Hour 2\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_test, df_test[\"Hour 2 Predispatch\"]))\n",
    "mae_2  = mean_absolute_error(y_test, df_test[\"Hour 2 Predispatch\"])\n",
    "\n",
    "# Hour 3\n",
    "rmse_3 = np.sqrt(mean_squared_error(y_test, df_test[\"Hour 3 Predispatch\"]))\n",
    "mae_3  = mean_absolute_error(y_test, df_test[\"Hour 3 Predispatch\"])\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Hour 2 Predispatch RMSE: {rmse_2:.2f} CAD/MWh\")\n",
    "print(f\"Hour 2 Predispatch MAE : {mae_2:.2f} CAD/MWh\")\n",
    "print(f\"Hour 3 Predispatch RMSE: {rmse_3:.2f} CAD/MWh\")\n",
    "print(f\"Hour 3 Predispatch MAE : {mae_3:.2f} CAD/MWh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "import pandas as pd  \n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Track the best across all k-values\n",
    "best_rmse = float('inf')\n",
    "best_n_hidden = None\n",
    "best_k = None\n",
    "best_selector = None\n",
    "best_selected_feature_names = None\n",
    "best_train_time = None\n",
    "best_scaler = None\n",
    "\n",
    "for k in [10, 20, 30, 40, 50, 60]:\n",
    "\n",
    "    # Step 1: Feature selection (on raw, unscaled data)\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    selector.fit(X_train, y_train)\n",
    "\n",
    "    # Step 2: Transform raw data with selector\n",
    "    X_train_sel_raw = selector.transform(X_train)\n",
    "    X_val_sel_raw   = selector.transform(X_val)\n",
    "    X_test_sel_raw  = selector.transform(X_test)\n",
    "\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_sel = scaler.fit_transform(X_train_sel_raw)\n",
    "    X_val_sel   = scaler.transform(X_val_sel_raw)\n",
    "    X_test_sel  = scaler.transform(X_test_sel_raw)\n",
    "\n",
    "    print(\"Hyperparameter Tuning Results:\")\n",
    "\n",
    "    for n_hidden in [100, 250, 500, 750]:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        W_in = np.random.randn(X_train_sel.shape[1], n_hidden)\n",
    "        H_train = np.maximum(0, X_train_sel @ W_in)\n",
    "        H_val = np.maximum(0, X_val_sel @ W_in)\n",
    "        W_out = np.linalg.pinv(H_train) @ y_train\n",
    "        \n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        y_val_pred = H_val @ W_out\n",
    "        val_rmse = np.sqrt(np.mean((y_val - y_val_pred)**2))\n",
    "        \n",
    "        print(f\"n_hidden={n_hidden}: Validation RMSE = {val_rmse:.2f}, Train Time = {train_time:.3f}s\")\n",
    "        \n",
    "        if val_rmse < best_rmse:\n",
    "            best_rmse = val_rmse\n",
    "            best_n_hidden = n_hidden\n",
    "            best_k = k\n",
    "            best_selector = selector\n",
    "            best_scaler = scaler\n",
    "            best_train_time = train_time\n",
    "            best_selected_feature_names = X_train.columns[selector.get_support()]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final Best ELM Model:\")\n",
    "print(f\"Best k (num features): {best_k}\")\n",
    "print(f\"Best n_hidden        : {best_n_hidden}\")\n",
    "print(f\"Best validation RMSE : {best_rmse:.2f}\")\n",
    "print(f\"Train Time           : {best_train_time:.3f}s\")\n",
    "\n",
    "print(\"\\nTop Features selected by SelectKBest (ELM):\")\n",
    "for i, feat in enumerate(best_selected_feature_names, 1):\n",
    "    print(f\"{i:2d}. {feat}\")\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X_train_sel_raw = best_selector.transform(X_train)\n",
    "X_test_sel_raw  = best_selector.transform(X_test)\n",
    "\n",
    "X_train_sel = best_scaler.fit_transform(X_train_sel_raw)\n",
    "X_test_sel  = best_scaler.transform(X_test_sel_raw)\n",
    "\n",
    "W_in = np.random.randn(X_train_sel.shape[1], best_n_hidden)\n",
    "H_train = np.maximum(0, X_train_sel @ W_in)\n",
    "H_test = np.maximum(0, X_test_sel @ W_in)\n",
    "W_out = np.linalg.pinv(H_train) @ y_train\n",
    "\n",
    "y_test_pred = H_test @ W_out\n",
    "\n",
    "# Metrics\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "mae  = mean_absolute_error(y_test, y_test_pred)\n",
    "r2   = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test MAE : {mae:.2f}\")\n",
    "print(f\"Test R²  : {r2:.3f}\")\n",
    "\n",
    "\n",
    "joblib.dump({\n",
    "    \"W_in\": W_in,\n",
    "    \"W_out\": W_out,\n",
    "    \"n_hidden\": best_n_hidden,\n",
    "    \"scaler\": best_scaler,\n",
    "    \"selector\": best_selector\n",
    "}, \"elm_model.pkl\")\n",
    "\n",
    "\n",
    "y_test_s = pd.Series(y_test, index=df_test.index)\n",
    "y_pred_s = pd.Series(y_test_pred, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349ca710",
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_windows = [3, 23, 167]\n",
    "ontario_holidays = holidays.Canada(prov='ON')\n",
    "\n",
    "\n",
    "df2 = final_df.copy()\n",
    "df2 = df2.reset_index()\n",
    "\n",
    "# Convert HOEP and predispatch to numeric\n",
    "df2['HOEP'] = pd.to_numeric(df2['HOEP'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df2['Hour 2 Predispatch'] = pd.to_numeric(df2['Hour 2 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df2['Hour 3 Predispatch'] = pd.to_numeric(df2['Hour 3 Predispatch'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "df2['timestamp'] = pd.to_datetime(df2['timestamp'])\n",
    "\n",
    "# ---------------- TIME FEATURES ----------------\n",
    "df2['hour'] = df2['timestamp'].dt.hour\n",
    "df2['day_of_week'] = df2['timestamp'].dt.dayofweek\n",
    "df2['month'] = df2['timestamp'].dt.month\n",
    "df2['year'] = df2['timestamp'].dt.year\n",
    "df2['quarter'] = df2['timestamp'].dt.quarter\n",
    "\n",
    "df2['hour_sin'] = np.sin(2 * np.pi * df2['hour'] / 24)\n",
    "df2['hour_cos'] = np.cos(2 * np.pi * df2['hour'] / 24)\n",
    "df2['is_weekend'] = (df2['day_of_week'] >= 5).astype(int)\n",
    "df2['month_sin'] = np.sin(2 * np.pi * df2['month'] / 12)\n",
    "df2['month_cos'] = np.cos(2 * np.pi * df2['month'] / 12)\n",
    "df2['week_of_year'] = df2['timestamp'].dt.isocalendar().week\n",
    "df2['week_of_year_sin'] = np.sin(2 * np.pi * df2['week_of_year'] / 52)\n",
    "df2['week_of_year_cos'] = np.cos(2 * np.pi * df2['week_of_year'] / 52)\n",
    "df2['day_of_year'] = df2['timestamp'].dt.dayofyear\n",
    "df2['doy_sin'] = np.sin(2 * np.pi * df2['day_of_year'] / 365)\n",
    "df2['doy_cos'] = np.cos(2 * np.pi * df2['day_of_year'] / 365)\n",
    "df2['is_holiday'] = df2['timestamp'].dt.date.isin(ontario_holidays).astype(int)\n",
    "df2['is_business_day'] = ((df2['day_of_week'] < 5) & (~df2['is_holiday'])).astype(int)\n",
    "df2['hour_of_week'] = df2['day_of_week'] * 24 + df2['hour']\n",
    "\n",
    "# Shift time features forward by 2 to match target at t+2\n",
    "time_cols = [\n",
    "    'hour', 'day_of_week', 'month', 'year', 'quarter',\n",
    "    'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
    "    'week_of_year', 'week_of_year_sin', 'week_of_year_cos',\n",
    "    'day_of_year', 'doy_sin', 'doy_cos',\n",
    "    'is_weekend', 'is_holiday', 'is_business_day',\n",
    "    'hour_of_week'\n",
    "]\n",
    "for col in time_cols:\n",
    "    df2[col] = df2[col].shift(-2)  # match forecast_horizon\n",
    "\n",
    "df2['Hour 2 Predispatch'] = df2['Hour 2 Predispatch'].shift(-2)\n",
    "df2['Hour 3 Predispatch'] = df2['Hour 3 Predispatch'].shift(-2)\n",
    "\n",
    "for win in roll_windows:\n",
    "    df2[f'demand_ma_{win}'] = df2['Ontario Demand'].rolling(win).mean()\n",
    "    df2[f'HOEP_ma_{win}'] = df2['HOEP'].rolling(win).mean()\n",
    "    \n",
    "    df2[f'temp_toronto_ma_{win}'] = df2['temp_toronto'].rolling(win).mean()\n",
    "    df2[f'humidity_toronto_ma_{win}'] = df2['humidity_toronto'].rolling(win).mean()\n",
    "    df2[f'wind_speed_toronto_ma_{win}'] = df2['wind_speed_toronto'].rolling(win).mean()\n",
    "\n",
    "df2['HOEP_volatility_24h'] = df2['HOEP'].rolling(23).std()\n",
    "df2['HOEP_range_24h'] = df2['HOEP'].rolling(23).max() - df2['HOEP'].rolling(23).min()\n",
    "\n",
    "\n",
    "df2['temp_toronto_squared'] = df2['temp_toronto'] ** 2\n",
    "df2['temp_ottawa_squared'] = df2['temp_ottawa'] ** 2\n",
    "df2['temp_kitchener_squared'] = df2['temp_kitchener'] ** 2\n",
    "df2['temp_london_squared'] = df2['temp_london'] ** 2\n",
    "\n",
    "df2 = df2.dropna()\n",
    "\n",
    "print(df2.shape)\n",
    "\n",
    "target_column = 'HOEP'\n",
    "\n",
    "df2[\"timestamp\"] = pd.to_datetime(df2[\"timestamp\"], utc=True)\n",
    "df2 = df2.set_index('timestamp').sort_index()\n",
    "\n",
    "all_features = df2.columns.difference([target_column]).tolist()\n",
    "# Ensure no duplicates\n",
    "lstm_features = list(dict.fromkeys(all_features))  # preserves order, removes duplicates\n",
    "\n",
    "print(f\"Number of features: {len(lstm_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08948ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM splits\n",
    "lstm_train = df2[df2.index < train_cutoff]\n",
    "lstm_val = df2[(df2.index >= train_cutoff) & (df2.index < val_cutoff)]\n",
    "lstm_test = df2[(df2.index >= val_cutoff) & (df2.index < final_cutoff)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c69917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Config\n",
    "n_steps = 6          # use last 6 hours\n",
    "forecast_horizon = 2   # predict t using data up to t-2\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "lstm_units = 64\n",
    "target_col = \"HOEP\"\n",
    "\n",
    "# Prepare features\n",
    "lstm_train = lstm_train.copy()\n",
    "lstm_val = lstm_val.copy()\n",
    "lstm_test = lstm_test.copy()\n",
    "\n",
    "# Shift HOEP by -2 to predict t+2\n",
    "lstm_train['y'] = lstm_train[target_col].shift(-forecast_horizon)\n",
    "lstm_val['y']   = lstm_val[target_col].shift(-forecast_horizon)\n",
    "lstm_test['y']  = lstm_test[target_col].shift(-forecast_horizon)\n",
    "\n",
    "\n",
    "lstm_train = lstm_train.dropna(subset=['y'])\n",
    "lstm_val = lstm_val.dropna(subset=['y'])\n",
    "lstm_test = lstm_test.dropna(subset=['y'])\n",
    "\n",
    "# Raw feature matrices\n",
    "X_train_lstm_raw = lstm_train[lstm_features].copy()\n",
    "X_val_lstm_raw = lstm_val[lstm_features].copy()\n",
    "X_test_lstm_raw = lstm_test[lstm_features].copy()\n",
    "\n",
    "y_train_lstm = lstm_train['y'].values\n",
    "y_val_lstm = lstm_val['y'].values\n",
    "y_test_lstm = lstm_test['y'].values\n",
    "\n",
    "# Scale all features\n",
    "lstm_scaler = StandardScaler()\n",
    "X_train_lstm = lstm_scaler.fit_transform(X_train_lstm_raw)\n",
    "X_val_lstm = lstm_scaler.transform(X_val_lstm_raw)\n",
    "X_test_lstm = lstm_scaler.transform(X_test_lstm_raw)\n",
    "\n",
    "# Target\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_lstm_scaled = y_scaler.fit_transform(y_train_lstm.reshape(-1, 1)).flatten()\n",
    "y_val_lstm_scaled   = y_scaler.transform(y_val_lstm.reshape(-1, 1)).flatten()\n",
    "y_test_lstm_scaled  = y_scaler.transform(y_test_lstm.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Build sequences\n",
    "\n",
    "X_train_seq, y_train_seq = [], []\n",
    "for i in range(n_steps, len(X_train_lstm)):\n",
    "    X_train_seq.append(X_train_lstm[i - n_steps:i])\n",
    "    y_train_seq.append(y_train_lstm_scaled[i])\n",
    "X_train_seq = np.array(X_train_seq)\n",
    "y_train_seq = np.array(y_train_seq)\n",
    "\n",
    "X_val_seq, y_val_seq = [], []\n",
    "for i in range(n_steps, len(X_val_lstm)):\n",
    "    X_val_seq.append(X_val_lstm[i - n_steps:i])\n",
    "    y_val_seq.append(y_val_lstm_scaled[i])\n",
    "X_val_seq = np.array(X_val_seq)\n",
    "y_val_seq = np.array(y_val_seq)\n",
    "\n",
    "X_test_seq, y_test_seq = [], []\n",
    "for i in range(n_steps, len(X_test_lstm)):\n",
    "    X_test_seq.append(X_test_lstm[i - n_steps:i])\n",
    "    y_test_seq.append(y_test_lstm_scaled[i])\n",
    "X_test_seq = np.array(X_test_seq)\n",
    "y_test_seq = np.array(y_test_seq)\n",
    "\n",
    "# Model architecture\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(n_steps, X_train_seq.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(32),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Train model\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = early_stop\n",
    "\n",
    "# Train model\n",
    "start_time = time.time()\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    shuffle=False,\n",
    "    verbose=1\n",
    ")\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Predict and evaluate\n",
    "\n",
    "y_pred_scaled = model.predict(X_test_seq).flatten()\n",
    "y_pred_lstm = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_true_lstm = y_scaler.inverse_transform(y_test_seq.reshape(-1, 1)).flatten()\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true_lstm, y_pred_lstm))\n",
    "mae = mean_absolute_error(y_true_lstm, y_pred_lstm)\n",
    "r2 = r2_score(y_true_lstm, y_pred_lstm)\n",
    "\n",
    "print(f\"Test RMSE     : {rmse:.2f}\")\n",
    "print(f\"Test MAE      : {mae:.2f}\")\n",
    "print(f\"Test R²       : {r2:.3f}\")\n",
    "print(f\"Train Time    : {train_time:.2f} seconds\")\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "# Save model\n",
    "model.save(\"lstm_model.h5\")\n",
    "\n",
    "# Save scaler\n",
    "dump(y_scaler, \"lstm_scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac96ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title(\"LSTM Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (Huber)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"loss_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# True vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_true_lstm[-200:], label='True HOEP', linewidth=2)\n",
    "plt.plot(y_pred_lstm[-200:], label='Predicted HOEP', linewidth=2)\n",
    "plt.title(\"True vs Predicted HOEP (Last 200 Samples)\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"HOEP ($/MWh)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hoep_vs_prediction.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
