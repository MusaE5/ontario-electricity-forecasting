{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "price_df = []\n",
    "demand_df = []\n",
    "\n",
    "for year in range(2018, 2026):\n",
    "\n",
    "    price_path  = fr'C:\\Users\\owner\\Downloads\\PUB_PriceHOEPPredispOR_{year}.csv'\n",
    "    demand_path = fr'C:\\Users\\owner\\Downloads\\PUB_Demand_{year}.csv'\n",
    "\n",
    "\n",
    "    df  = pd.read_csv(price_path,  skiprows=3)\n",
    "    price_df.append(df)\n",
    "    df = pd.read_csv(demand_path, skiprows =3)\n",
    "    demand_df.append(df)\n",
    "\n",
    "price_df = pd.concat(price_df, ignore_index= True)\n",
    "demand_df = pd.concat(demand_df, ignore_index= True)\n",
    "\n",
    "# Clean column names\n",
    "price_df.columns = price_df.columns.str.strip()\n",
    "demand_df.columns = demand_df.columns.str.strip()\n",
    "\n",
    "# Build a timestamp\n",
    "price_df['Date'] = pd.to_datetime(price_df['Date'], format= '%Y-%m-%d')\n",
    "demand_df['Date'] = pd.to_datetime(demand_df['Date'], format='%Y-%m-%d')\n",
    "\n",
    "price_df['timestamp'] = price_df['Date'] + pd.to_timedelta(price_df[\"Hour\"]-1, unit = 'h')\n",
    "demand_df['timestamp'] = demand_df['Date'] + pd.to_timedelta(demand_df['Hour'] - 1, unit='h')\n",
    "\n",
    "demand_df = demand_df.drop(columns=[\"Date\", \"Hour\"])  # keep only 1 version\n",
    "\n",
    "demand_df = demand_df.rename(columns={\"Ontario Demand\": \"Demand\"})\n",
    "\n",
    "\n",
    "# Merge on timestamp\n",
    "combined_df = pd.merge(price_df, demand_df, on='timestamp', how='inner')\n",
    "\n",
    " # Forward fill \n",
    "combined_df['Hour 1 Predispatch'] = combined_df['Hour 1 Predispatch'].fillna(method='ffill')\n",
    "    \n",
    "combined_df['Hour 2 Predispatch'] = combined_df['Hour 2 Predispatch'].fillna(method='ffill')\n",
    "\n",
    "combined_df['Hour 3 Predispatch'] = combined_df['Hour 3 Predispatch'].fillna(method='ffill')\n",
    "\n",
    "# Quick check\n",
    "display(combined_df.head())     # see first 5 rows\n",
    "print(combined_df.info())       # check for nulls & types\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Price over time\n",
    "plt.figure()\n",
    "combined_df.set_index('timestamp')['HOEP'].plot(title='HOEP Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price (CAD/MWh)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Demand vs. Price scatter\n",
    "plt.figure()\n",
    "plt.scatter(combined_df['Demand'], combined_df['HOEP'], s=5)\n",
    "plt.title('HOEP vs. Demand')\n",
    "plt.xlabel('Demand (MW)')\n",
    "plt.ylabel('Price (CAD/MWh)')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207437d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "weather_path  = r'C:\\Users\\owner\\Downloads\\weather'\n",
    "\n",
    "weather_files = [f for f in os.listdir(weather_path)]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for file in sorted(weather_files):\n",
    "    file_path = os.path.join(weather_path, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "\n",
    "weather_df = pd.concat(dfs, ignore_index = True)  \n",
    "\n",
    "# Clean column names \n",
    "weather_df.columns = weather_df.columns.str.strip().str.replace('\"', '')\n",
    "\n",
    "# Rename to clean and parse timestamp\n",
    "weather_df = weather_df.rename(columns={\"Date/Time (LST)\": \"timestamp\"})\n",
    "\n",
    "\n",
    "# Parse datetime\n",
    "weather_df[\"timestamp\"] = pd.to_datetime(weather_df[\"timestamp\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Keep relevant features only\n",
    "cols_to_keep = {\n",
    "    \"timestamp\": \"timestamp\",\n",
    "    \"Temp (Â°C)\": \"temp\",\n",
    "    \"Rel Hum (%)\": \"humidity\",\n",
    "    \"Wind Spd (km/h)\": \"wind_speed\"\n",
    " }\n",
    "\n",
    "\n",
    "weather_df = weather_df[list(cols_to_keep.keys())].rename(columns=cols_to_keep)\n",
    "weather_df = weather_df.set_index(\"timestamp\")\n",
    "\n",
    "\n",
    "weather_df = weather_df.astype(\"float32\")\n",
    "\n",
    "# Sort and forward fill missing weather values\n",
    "weather_df = weather_df.sort_index()\n",
    "weather_df[[\"temp\", \"humidity\", \"wind_speed\"]] = weather_df[[\"temp\", \"humidity\", \"wind_speed\"]].ffill()\n",
    "\n",
    "\n",
    "\n",
    "display(weather_df.head())     # see first 5 rows\n",
    "print(weather_df.info())  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f422ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_df = pd.merge(combined_df, weather_df, on='timestamp', how='inner')\n",
    "\n",
    "print(merged_df.head())\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import holidays \n",
    "\n",
    "# Extended lag and window configurations\n",
    "lags = [2, 3, 24, 48, 168]  # Added 48h and 168h (weekly)\n",
    "roll_windows = [3, 24, 168]  # Added weekly rolling window\n",
    "\n",
    "df = merged_df.copy()\n",
    "\n",
    "# Create Ontario holidays object\n",
    "ontario_holidays = holidays.Canada(prov='ON')\n",
    "\n",
    "# Enhanced Time features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.hour / 24)\n",
    "df['is_weekend'] = (df['timestamp'].dt.weekday >= 5).astype(int)\n",
    "\n",
    "# Enhanced temporal features\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['timestamp'].dt.month / 12)\n",
    "df['week_of_year'] = df['timestamp'].dt.isocalendar().week\n",
    "df['week_of_year_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_of_year_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "\n",
    "# Day of year features\n",
    "df['day_of_year'] = df['timestamp'].dt.dayofyear\n",
    "df['doy_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365)\n",
    "df['doy_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365)\n",
    "\n",
    "# Holiday and business day features\n",
    "df['is_holiday'] = df['timestamp'].dt.date.isin(ontario_holidays)\n",
    "df['is_business_day'] = ((df['timestamp'].dt.weekday < 5) & (~df['is_holiday'])).astype(int)\n",
    "\n",
    "# Hour of week (combines day and hour patterns)\n",
    "df['hour_of_week'] = df['timestamp'].dt.weekday * 24 + df['timestamp'].dt.hour\n",
    "\n",
    "# Lagged features with no leakage\n",
    "for k in lags:\n",
    "    df[f'demand_lag_{k}'] = df['Demand'].shift(k)\n",
    "    df[f'temp_lag_{k}'] = df['temp'].shift(k)\n",
    "    df[f'humidity_lag_{k}'] = df['humidity'].shift(k)\n",
    "    df[f'wind_speed_lag_{k}'] = df['wind_speed'].shift(k)\n",
    "    df[f'HOEP_lag_{k}'] = df['HOEP'].shift(k)\n",
    "    df[f'OR_30_Min_lag_{k}'] = df['OR 30 Min'].shift(k)\n",
    "    df[f'OR_10_Min_sync_lag{k}'] = df['OR 10 Min Sync'].shift(k)\n",
    "    df[f'OR_10_Min_non-sync_lag{k}'] = df['OR 10 Min non-sync'].shift(k)\n",
    "\n",
    "# Enhanced weather features (derived from existing weather data)\n",
    "df['temp_change_24h'] = df['temp_lag_2'] - df['temp_lag_24']\n",
    "df['humidity_change_24h'] = df['humidity_lag_2'] - df['humidity_lag_24']\n",
    "df['wind_speed_change_24h'] = df['wind_speed_lag_2'] - df['wind_speed_lag_24']\n",
    "\n",
    "# Non-linear temperature effects\n",
    "df['temp_squared_lag_2'] = df['temp_lag_2'] ** 2\n",
    "\n",
    "# OR features (operating reserve indicators)\n",
    "df['total_OR_lag_2'] = (df['OR_30_Min_lag_2'] + \n",
    "                        df['OR_10_Min_sync_lag2'] + \n",
    "                        df['OR_10_Min_non-sync_lag2'])\n",
    "df['OR_ratio_lag_2'] = df['OR_10_Min_sync_lag2'] / (df['OR_30_Min_lag_2'] + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Rolling features (moving averages)\n",
    "for win in roll_windows:\n",
    "    df[f'demand_ma_{win}'] = df['demand_lag_2'].rolling(win).mean()\n",
    "    df[f'temp_ma_{win}'] = df['temp_lag_2'].rolling(win).mean()\n",
    "    df[f'humidity_ma_{win}'] = df['humidity_lag_2'].rolling(win).mean()\n",
    "    df[f'wind_speed_ma_{win}'] = df['wind_speed_lag_2'].rolling(win).mean()\n",
    "    df[f'HOEP_ma_{win}'] = df['HOEP_lag_2'].rolling(win).mean()\n",
    "\n",
    "# Volatility features (market stress indicators)\n",
    "df['HOEP_volatility_24h'] = df['HOEP_lag_2'].rolling(24).std()\n",
    "df['HOEP_range_24h'] = (df['HOEP_lag_2'].rolling(24).max() - \n",
    "                        df['HOEP_lag_2'].rolling(24).min())\n",
    "\n",
    "# High-value interaction features\n",
    "df['demand_temp_interaction'] = df['demand_lag_2'] * df['temp_lag_2']\n",
    "df['hour_weekend_interaction'] = df['timestamp'].dt.hour * df['is_weekend']\n",
    "df['temp_humidity_interaction'] = df['temp_lag_2'] * df['humidity_lag_2']\n",
    "df['demand_HOEP_ratio_lag_2'] = df['demand_lag_2'] / (df['HOEP_lag_2'] + 1e-6)\n",
    "\n",
    "# Holiday proximity features (optional - can be computationally expensive)\n",
    "def add_holiday_proximity(df, max_days=7):\n",
    "    \"\"\"Add days to/from nearest holiday (within max_days)\"\"\"\n",
    "    df['days_to_holiday'] = max_days + 1\n",
    "    df['days_from_holiday'] = max_days + 1\n",
    "    \n",
    "    for i, date in enumerate(df['timestamp'].dt.date):\n",
    "        # Find holidays within range\n",
    "        date_range = pd.date_range(date - pd.Timedelta(days=max_days), \n",
    "                                 date + pd.Timedelta(days=max_days))\n",
    "        nearby_holidays = [d.date() for d in date_range if d.date() in ontario_holidays]\n",
    "        \n",
    "        if nearby_holidays:\n",
    "            future_holidays = [h for h in nearby_holidays if h > date]\n",
    "            past_holidays = [h for h in nearby_holidays if h < date]\n",
    "            \n",
    "            if future_holidays:\n",
    "                df.iloc[i, df.columns.get_loc('days_to_holiday')] = min([(h - date).days for h in future_holidays])\n",
    "            if past_holidays:\n",
    "                df.iloc[i, df.columns.get_loc('days_from_holiday')] = min([(date - h).days for h in past_holidays])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Uncomment if you want holiday proximity (adds computation time)\n",
    "# df = add_holiday_proximity(df)\n",
    "\n",
    "# Drop NaN values (due to lags and rolling windows)\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"=== Enhanced Feature Engineering Results ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Earliest timestamp: {df['timestamp'].min()}\")\n",
    "print(f\"Latest timestamp: {df['timestamp'].max()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Feature categories for easy reference\n",
    "temporal_features = ['hour_sin', 'hour_cos', 'is_weekend', 'month_sin', 'month_cos', \n",
    "                    'week_of_year_sin', 'week_of_year_cos', 'day_of_year', 'doy_sin', 'doy_cos',\n",
    "                    'is_holiday', 'is_business_day', 'hour_of_week']\n",
    "\n",
    "lag_features = [col for col in df.columns if '_lag_' in col]\n",
    "rolling_features = [col for col in df.columns if '_ma_' in col or 'volatility' in col or 'range' in col]\n",
    "interaction_features = [col for col in df.columns if 'interaction' in col or 'ratio' in col]\n",
    "weather_derived = ['temp_change_24h', 'humidity_change_24h', 'wind_speed_change_24h', 'temp_squared_lag_2']\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"Temporal features: {len(temporal_features)}\")\n",
    "print(f\"Lag features: {len(lag_features)}\")\n",
    "print(f\"Rolling features: {len(rolling_features)}\")\n",
    "print(f\"Interaction features: {len(interaction_features)}\")\n",
    "print(f\"Weather derived: {len(weather_derived)}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "\n",
    "# Updated feature list for your model\n",
    "features = (temporal_features + lag_features + rolling_features + \n",
    "                    interaction_features + weather_derived)\n",
    "\n",
    "print(f\"\\nEnhanced feature list length: {len(features)}\")\n",
    "print(\"\\nFirst 10 enhanced features:\")\n",
    "for i, feat in enumerate(features[:10]):\n",
    "    print(f\"  {i+1}. {feat}\")\n",
    "\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c6f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "# Define quantile loss function\n",
    "def quantile_loss(q):\n",
    "    \"\"\"Custom loss function for quantile regression\"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        return tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n",
    "    return loss\n",
    "\n",
    "# Define multiple quantiles to predict\n",
    "quantiles = [0.1, 0.5, 0.9]  # 10th, 50th (median), 90th percentiles\n",
    "\n",
    "# Method 1: Single model predicting multiple quantiles\n",
    "def create_multi_quantile_model(input_shape, quantiles):\n",
    "    \"\"\"Create model that predicts multiple quantiles simultaneously\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(len(quantiles), activation='linear')  # One output per quantile\n",
    "    ])\n",
    "    \n",
    "    # Custom combined loss for multiple quantiles\n",
    "    def combined_quantile_loss(y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i, q in enumerate(quantiles):\n",
    "            q_pred = y_pred[:, i:i+1]\n",
    "            error = y_true - q_pred\n",
    "            total_loss += tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n",
    "        return total_loss\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.001), loss=combined_quantile_loss)\n",
    "    return model\n",
    "\n",
    "# Method 2: Separate models for each quantile (often works better)\n",
    "def create_single_quantile_model(input_shape, quantile):\n",
    "    \"\"\"Create model for single quantile prediction\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=(input_shape,)),\n",
    "        Dense(128),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(64),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dropout(0.2),\n",
    "        Dense(32),\n",
    "        LeakyReLU(alpha=0.01),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(0.001), loss=quantile_loss(quantile))\n",
    "    return model\n",
    "\n",
    "# Training function for quantile regression\n",
    "def train_quantile_models(X_train, y_train, X_test, y_test, method='separate'):\n",
    "    \"\"\"Train quantile regression models\"\"\"\n",
    "    \n",
    "    if method == 'combined':\n",
    "        # Method 1: Single model for all quantiles\n",
    "        model = create_multi_quantile_model(X_train.shape[1], quantiles)\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=32,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Predict all quantiles\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "        \n",
    "        # Separate predictions by quantile\n",
    "        quantile_predictions = {}\n",
    "        for i, q in enumerate(quantiles):\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = predictions[:, i]\n",
    "            \n",
    "    else:\n",
    "        # Method 2: Separate models for each quantile (recommended)\n",
    "        quantile_models = {}\n",
    "        quantile_predictions = {}\n",
    "        \n",
    "        for q in quantiles:\n",
    "            print(f\"Training quantile {q} model...\")\n",
    "            \n",
    "            model = create_single_quantile_model(X_train.shape[1], q)\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "            \n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.1,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Store model and predictions\n",
    "            quantile_models[f'q_{int(q*100)}'] = model\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    return quantile_predictions\n",
    "\n",
    "# Evaluation metrics for quantile regression\n",
    "def evaluate_quantile_predictions(y_true, quantile_predictions):\n",
    "    \"\"\"Evaluate quantile regression performance\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for q_name, q_pred in quantile_predictions.items():\n",
    "        q_value = float(q_name.split('_')[1]) / 100\n",
    "        \n",
    "        # Pinball loss (quantile loss)\n",
    "        error = y_true - q_pred\n",
    "        pinball_loss = np.mean(np.maximum(q_value * error, (q_value - 1) * error))\n",
    "        \n",
    "        # Coverage (what % of actual values fall below this quantile)\n",
    "        coverage = np.mean(y_true <= q_pred)\n",
    "        \n",
    "        # RMSE for this quantile\n",
    "        rmse = np.sqrt(np.mean((y_true - q_pred)**2))\n",
    "        \n",
    "        results[q_name] = {\n",
    "            'pinball_loss': pinball_loss,\n",
    "            'coverage': coverage,\n",
    "            'expected_coverage': q_value,\n",
    "            'rmse': rmse\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Prediction interval metrics\n",
    "def calculate_prediction_intervals(quantile_predictions, confidence_levels=[0.8]):\n",
    "    \"\"\"Calculate prediction intervals from quantile predictions\"\"\"\n",
    "    intervals = {}\n",
    "    \n",
    "    for conf_level in confidence_levels:\n",
    "        alpha = 1 - conf_level\n",
    "        lower_q = f\"q_{int((alpha/2)*100)}\"\n",
    "        upper_q = f\"q_{int((1-alpha/2)*100)}\"\n",
    "        \n",
    "        if lower_q in quantile_predictions and upper_q in quantile_predictions:\n",
    "            intervals[f'{int(conf_level*100)}%'] = {\n",
    "                'lower': quantile_predictions[lower_q],\n",
    "                'upper': quantile_predictions[upper_q],\n",
    "                'width': quantile_predictions[upper_q] - quantile_predictions[lower_q]\n",
    "            }\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "# Visualization function\n",
    "def plot_quantile_predictions(y_true, quantile_predictions, start_idx=0, end_idx=200):\n",
    "    \"\"\"Plot quantile predictions with uncertainty bands\"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot actual values\n",
    "    x = range(start_idx, min(end_idx, len(y_true)))\n",
    "    plt.plot(x, y_true[start_idx:end_idx], 'k-', label='Actual HOEP', linewidth=2)\n",
    "    \n",
    "    # Plot median prediction\n",
    "    if 'q_50' in quantile_predictions:\n",
    "        plt.plot(x, quantile_predictions['q_50'][start_idx:end_idx], 'r--', \n",
    "                label='Median Prediction', linewidth=2)\n",
    "    \n",
    "    # Plot uncertainty bands\n",
    "    if 'q_10' in quantile_predictions and 'q_90' in quantile_predictions:\n",
    "        plt.fill_between(x, \n",
    "                        quantile_predictions['q_10'][start_idx:end_idx],\n",
    "                        quantile_predictions['q_90'][start_idx:end_idx],\n",
    "                        alpha=0.3, color='blue', label='80% Prediction Interval')\n",
    "    \n",
    "    plt.xlabel('Time (hours)')\n",
    "    plt.ylabel('HOEP (CAD/MWh)')\n",
    "    plt.title('Quantile Regression Predictions with Uncertainty')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c3852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time based, no Train test split\n",
    "df_train = df[df[\"timestamp\"] < \"2024-01-01\"]\n",
    "df_test  = df[df[\"timestamp\"] >= \"2024-01-01\"]\n",
    "\n",
    "target = \"HOEP\"\n",
    "\n",
    "\n",
    "X_train_raw = df_train[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_train     = pd.to_numeric(df_train[target], errors=\"coerce\")\n",
    "X_test_raw  = df_test[features].apply(pd.to_numeric, errors=\"coerce\")\n",
    "y_test      = pd.to_numeric(df_test[target], errors=\"coerce\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "X_test  = scaler.transform(X_test_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "print(\"Training quantile regression models...\")\n",
    "quantile_predictions = train_quantile_models(X_train, y_train, X_test, y_test, method='separate')\n",
    "    \n",
    "print(\"\\nEvaluating quantile predictions...\")\n",
    "results = evaluate_quantile_predictions(y_test, quantile_predictions)\n",
    "    \n",
    "print(\"\\n=== Quantile Regression Results ===\")\n",
    "for q_name, metrics in results.items():\n",
    "    print(f\"\\nQuantile {q_name}:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"  Pinball Loss: {metrics['pinball_loss']:.2f}\")\n",
    "    print(f\"  Coverage: {metrics['coverage']:.3f} (expected: {metrics['expected_coverage']:.3f})\")\n",
    "    \n",
    "# Calculate prediction intervals\n",
    "intervals = calculate_prediction_intervals(quantile_predictions)\n",
    "    \n",
    "print(\"\\n=== Prediction Intervals ===\")\n",
    "for interval_name, interval_data in intervals.items():\n",
    "    avg_width = np.mean(interval_data['width'])\n",
    "    print(f\"{interval_name} interval average width: {avg_width:.2f} CAD/MWh\")\n",
    "    \n",
    "# Plot results\n",
    "plot_quantile_predictions(y_test, quantile_predictions)\n",
    "    \n",
    "print(f\"Median quantile RMSE: {results['q_50']['rmse']:.2f}\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702556f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Reuse your existing quantiles and evaluation functions\n",
    "# quantiles = [0.1, 0.5, 0.9] (already defined)\n",
    "# evaluate_quantile_predictions() (already defined)\n",
    "# calculate_prediction_intervals() (already defined)\n",
    "# plot_quantile_predictions() (already defined)\n",
    "\n",
    "def train_xgb_quantile_models(X_train_raw, y_train, X_test_raw, y_test):\n",
    "    \"\"\"Train XGBoost quantile models - minimal version for jupyter\"\"\"\n",
    "    \n",
    "    quantile_predictions = {}\n",
    "    \n",
    "    print(\"Training XGBoost quantile models...\")\n",
    "    \n",
    "    for q in quantiles:\n",
    "        print(f\"Training XGBoost quantile {q} model...\")\n",
    "        \n",
    "        # Create XGBoost model for this quantile\n",
    "        model = xgb.XGBRegressor(\n",
    "            objective='reg:quantileerror',\n",
    "            quantile_alpha=q,\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        # Train on raw data (no scaling needed)\n",
    "        model.fit(X_train_raw, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        quantile_predictions[f'q_{int(q * 100)}'] = model.predict(X_test_raw)\n",
    "    \n",
    "    return quantile_predictions\n",
    "\n",
    "X_train_raw = X_train_raw.values  # Convert DataFrame to numpy\n",
    "X_test_raw = X_test_raw.values    # Convert DataFrame to numpy\n",
    "y_train = y_train.values          # Convert Series to numpy  \n",
    "y_test = y_test.values            # Convert Series to numpy\n",
    "\n",
    "# Train XGBoost models (use your raw X_train, X_test - not scaled versions)\n",
    "xgb_predictions = train_xgb_quantile_models(X_train_raw, y_train, X_test_raw, y_test)\n",
    "\n",
    "# Evaluate using your existing functions\n",
    "print(\"\\nEvaluating XGBoost quantile predictions...\")\n",
    "xgb_results = evaluate_quantile_predictions(y_test, xgb_predictions)\n",
    "\n",
    "print(\"\\n=== XGBoost Quantile Regression Results ===\")\n",
    "for q_name, metrics in xgb_results.items():\n",
    "    print(f\"\\nQuantile {q_name}:\")\n",
    "    print(f\"  RMSE: {metrics['rmse']:.2f}\")\n",
    "    print(f\"  Pinball Loss: {metrics['pinball_loss']:.2f}\")\n",
    "    print(f\"  Coverage: {metrics['coverage']:.3f} (expected: {metrics['expected_coverage']:.3f})\")\n",
    "\n",
    "# Calculate prediction intervals\n",
    "xgb_intervals = calculate_prediction_intervals(xgb_predictions)\n",
    "\n",
    "print(\"\\n=== XGBoost Prediction Intervals ===\")\n",
    "for interval_name, interval_data in xgb_intervals.items():\n",
    "    avg_width = np.mean(interval_data['width'])\n",
    "    print(f\"{interval_name} interval average width: {avg_width:.2f} CAD/MWh\")\n",
    "\n",
    "# Plot results\n",
    "plot_quantile_predictions(y_test, xgb_predictions)\n",
    "\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"XGBoost q_50 RMSE: {xgb_results['q_50']['rmse']:.2f}\")\n",
    "print(f\"Neural Network q_50 RMSE: 33.92\")  # Your NN result\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b8d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Input, LayerNormalization, MultiHeadAttention, Add\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"Shallow transformer block with multi-head attention\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Self-attention\n",
    "        attn_output = self.att(inputs, inputs, training=training)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_transformer_quantile_model(input_shape, quantile, embed_dim=64, num_heads=4, ff_dim=128):\n",
    "    \"\"\"Create shallow transformer model for single quantile prediction\"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # Initial projection to embedding dimension\n",
    "    x = Dense(embed_dim)(inputs)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    # Reshape for transformer (add sequence dimension)\n",
    "    x = tf.expand_dims(x, axis=1)  # Shape: (batch, 1, embed_dim)\n",
    "    \n",
    "    # Shallow transformer block (just one layer)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate=0.2)\n",
    "    x = transformer_block(x, training=True)\n",
    "    \n",
    "    # Global pooling (since we have sequence length of 1)\n",
    "    x = tf.squeeze(x, axis=1)  # Shape: (batch, embed_dim)\n",
    "    \n",
    "    # Final prediction layers\n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1)(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss=quantile_loss(quantile))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def create_multi_output_transformer_model(input_shape, quantiles, embed_dim=64, num_heads=4, ff_dim=128):\n",
    "    \"\"\"Create transformer model that predicts multiple quantiles simultaneously\"\"\"\n",
    "    \n",
    "    inputs = Input(shape=(input_shape,))\n",
    "    \n",
    "    # Initial projection to embedding dimension\n",
    "    x = Dense(embed_dim)(inputs)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    \n",
    "    # Reshape for transformer\n",
    "    x = tf.expand_dims(x, axis=1)\n",
    "    \n",
    "    # Shallow transformer block\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate=0.2)\n",
    "    x = transformer_block(x, training=True)\n",
    "    \n",
    "    # Global pooling\n",
    "    x = tf.squeeze(x, axis=1)\n",
    "    \n",
    "    # Final prediction layers\n",
    "    x = Dense(32)(x)\n",
    "    x = LeakyReLU(alpha=0.01)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(len(quantiles))(x)  # One output per quantile\n",
    "    \n",
    "    # Custom combined loss for multiple quantiles\n",
    "    def combined_quantile_loss(y_true, y_pred):\n",
    "        total_loss = 0\n",
    "        for i, q in enumerate(quantiles):\n",
    "            q_pred = y_pred[:, i:i+1]\n",
    "            error = y_true - q_pred\n",
    "            total_loss += tf.reduce_mean(tf.maximum(q * error, (q - 1) * error))\n",
    "        return total_loss\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=Adam(0.001), loss=combined_quantile_loss)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_transformer_quantile_models(X_train, y_train, X_test, y_test, method='separate'):\n",
    "    \"\"\"\n",
    "    Train transformer-based quantile regression models\n",
    "    This is a wrapper that creates transformer models but uses the existing\n",
    "    train_quantile_models logic where possible\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure y_train is reshaped properly\n",
    "    if len(y_train.shape) == 1:\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "    \n",
    "    if method == 'combined':\n",
    "        # Single model for all quantiles\n",
    "        print(\"Training combined transformer model for all quantiles...\")\n",
    "        model = create_multi_output_transformer_model(X_train.shape[1], quantiles)\n",
    "        \n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        \n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_split=0.1,\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Predict all quantiles\n",
    "        predictions = model.predict(X_test, verbose=0)\n",
    "        \n",
    "        # Separate predictions by quantile\n",
    "        quantile_predictions = {}\n",
    "        for i, q in enumerate(quantiles):\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = predictions[:, i]\n",
    "            \n",
    "    else:\n",
    "        # Separate models for each quantile (recommended)\n",
    "        quantile_models = {}\n",
    "        quantile_predictions = {}\n",
    "        \n",
    "        for q in quantiles:\n",
    "            print(f\"\\nTraining transformer model for quantile {q}...\")\n",
    "            \n",
    "            model = create_transformer_quantile_model(X_train.shape[1], q)\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_split=0.1,\n",
    "                epochs=100,\n",
    "                batch_size=64,\n",
    "                callbacks=[early_stop],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Store model and predictions\n",
    "            quantile_models[f'q_{int(q*100)}'] = model\n",
    "            quantile_predictions[f'q_{int(q*100)}'] = model.predict(X_test, verbose=0).flatten()\n",
    "    \n",
    "    return quantile_predictions, quantile_models if method == 'separate' else None\n",
    "\n",
    "# Main execution example\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming X_train, y_train, X_test, y_test are already prepared as in your code\n",
    "    # and the original functions (quantile_loss, evaluate_quantile_predictions, \n",
    "    # plot_quantile_predictions, calculate_prediction_intervals) are available\n",
    "    \n",
    "    # Train transformer models with separate quantiles (recommended)\n",
    "    print(\"Training Transformer models for HOEP 1-hour ahead forecasting...\")\n",
    "    quantile_predictions, models = train_transformer_quantile_models(\n",
    "        X_train, y_train, X_test, y_test, method='separate'\n",
    "    )\n",
    "    \n",
    "    # Use the existing evaluation function\n",
    "    print(\"\\nEvaluating quantile predictions...\")\n",
    "    evaluation_results = evaluate_quantile_predictions(y_test, quantile_predictions)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nQuantile Regression Performance:\")\n",
    "    print(\"-\" * 60)\n",
    "    for q_name, metrics in evaluation_results.items():\n",
    "        print(f\"\\n{q_name.upper()}:\")\n",
    "        print(f\"  Pinball Loss: {metrics['pinball_loss']:.4f}\")\n",
    "        print(f\"  Coverage: {metrics['coverage']:.3f} (Expected: {metrics['expected_coverage']:.3f})\")\n",
    "        print(f\"  RMSE: {metrics['rmse']:.4f}\")\n",
    "    \n",
    "    # Use the existing calculate_prediction_intervals function\n",
    "    intervals = calculate_prediction_intervals(quantile_predictions)\n",
    "    if '80%' in intervals:\n",
    "        print(f\"\\n80% Prediction Interval Average Width: {np.mean(intervals['80%']['width']):.2f}\")\n",
    "    \n",
    "    # Use the existing plot function\n",
    "    plot_quantile_predictions(y_test, quantile_predictions, start_idx=0, end_idx=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
